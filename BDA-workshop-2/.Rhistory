abline(v=max(log.extinct), col="red", lwd=2)
hist(yrepmax, col="grey40")
abline(v=min(log.extinct), col="red")
yrep <- matrix(0, nrow=n, ncol=nsamp)
for (i in 1: nsamp) {
yrep[, i] <- rnorm(n, mean=fittedvalues[, i], sd=sigma[i])
}
yrepmin <- apply(yrep, 1, min)
yrepmax <- apply(yrep, 1, max)
par(mfrow=c(2, 2))
hist(yrepmin, col="grey40")
abline(v=max(log.extinct), col="red", lwd=2)
hist(yrepmax, col="grey40")
abline(v=min(log.extinct), col="red")
yrep <- matrix(0, nrow=n, ncol=nsamp)
for (i in 1: nsamp) {
yrep[, i] <- rnorm(n, mean=fittedvalues[, i], sd=sigma[i])
}
yrepmin <- apply(yrep, 1, min)
yrepmax <- apply(yrep, 1, max)
par(mfrow=c(2, 2))
hist(yrepmin, col="grey40")
abline(v=max(log.extinct), col="red")
hist(yrepmax, col="grey40")
abline(v=min(log.extinct), col="red")
library(rjags)
#If it ran correctly, you should see
#Loading required package: coda
#Linked to JAGS 4.3.1
#Loaded modules: basemod,bugs
library(Stat2Data)
#If not available, please use install.packages("Stat2Data")
data("LongJumpOlympics")   #Makes the dataset available in this R session
Jump <- LongJumpOlympics$Gold
Year <- LongJumpOlympics$Year
#You can get more info about the dataset by the command
#help(topic="LongJumpOlympics",package="Stat2Data")
#1.1. Plot Jump vs Year
par(mfrow=c(1,1))
plot(Jump ~ Year,xlab="Year",ylab="Jump (m)",
type="n",main="Olympic Gold LongJump Distances (m)",cex=0.6)
text(x=Year,y=Jump,labels=Year)
#1.2. Fit simple linear regression model.
m1 <- lm(Jump ~ Year)
summary(m1)
#Plot data with fit overlaid.
par(mfrow=c(1,1))
plot(Jump ~ Year,xlab="Year",ylab="Jump (m)",
type="n",main="Olympic Gold LongJump Distances (m)",cex=0.6)
text(x=Year,y=Jump,labels=Year)
abline(m1,col="blue")
#1.3. Compute change in winning distance every 4 years
cat("Predicted change every 4 years=",coef(m1)["Year"]*4,"meters \n")
#1.4. Plot residuals against Year
plot(residuals(m1) ~ Year,xlab="Year",ylab="Residuals")
abline(h=0,col="red")
par(mfrow=c(2,2)); plot(m1); par(mfrow=c(1,1))
plot(m1,which=3)
#1.6. Remove outlier and refit
omit <- Year==1968
m2 <- lm(Jump[!omit] ~ Year[!omit])
par(mfrow=c(2,2))
plot(m2)
par(mfrow=c(1,1))
summary(m2)
#1.7. Create model block for JAGS
Jump.model <-
"model {
# data that will be read in are n,  Jump, Year and prior hyperparameters
# prior
beta0 ~ dnorm(beta.mu.0,beta.tau.0)
beta1 ~ dnorm(beta.mu.0,beta.tau.0)
tau   ~ dgamma(tau.a,tau.b)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta1*Year[i]
Jump[i] ~ dnorm(mu[i],tau)
}
sigma <- 1/sqrt(tau)
}"
# 2.8. Create data input for JAGS
n     <- length(Year)
Jump.data <- list(n=n,Jump=Jump,Year=Year,beta.mu.0=0,beta.tau.0=0.001,tau.a=0.1,tau.b=0.1)
# 2.9. Create initial values for JAGS
num.chains <- 3
Jump.inits <- list(list(beta0=0.2, beta1=0.1, tau=0.1),
list(beta0=4.0, beta1=0.0, tau=0.01),
list(beta0=-3.0,beta1=-2.0,tau=0.001))
# 2.10. Execute JAGS model
# Run JAGS to the completion of the "adaption" stage
results.A <- jags.model(file=textConnection(Jump.model),
data=Jump.data, inits=Jump.inits,
n.chains=num.chains)
# 2.11. Run burn-in
update(results.A, n.iter=1000)
# 2.12. Get production samples
results.B <- coda.samples(results.A, variable.names=c("beta0","beta1","sigma"),n.iter=10000)
# 2.13. Plot the results to check for convergence
# Check for convergence before looking at posterior dist'n summaries
par(mfrow=c(1,1))
plot(results.B)
#1.14. Evaluate autocorrelation functions
par(mfrow=c(1,3))
acf(results.B[[1]][,"beta0"],lag.max=100)
acf(results.B[[1]][,"beta1"],lag.max=100)
acf(results.B[[1]][,"sigma"],lag.max=100)
#1.15. Evaluate effective sample sizes for each parameter.
effectiveSize(results.B[[1]][,"beta0"])
effectiveSize(results.B[[1]][,"beta1"])
effectiveSize(results.B[[1]][,"sigma"])
# 2.16. Compute the Gelman-Rubin statistic
par(mfrow=c(2,2))
gelman.plot(results.B)
gelman.diag(results.B)
#1.17. Work with centered covariate to see if that helps----
Jump.ctr.model <-
"model {
# prior
beta0 ~ dnorm(beta.mu.0,beta.tau.0)
beta1 ~ dnorm(beta.mu.0,beta.tau.0)
tau   ~ dgamma(tau.a,tau.b)
meanY <- mean(Year)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta1*(Year[i]-meanY)
Jump[i] ~ dnorm(mu[i],tau)
}
sigma <- 1/sqrt(tau)
}"
# Run JAGS to the completion of the "adaption" stage
results.A.ctr <- jags.model(file=textConnection(Jump.ctr.model),
data=Jump.data, inits=Jump.inits,
n.chains=num.chains)
update(results.A.ctr, n.iter=1000)
results.B.ctr <- coda.samples(results.A.ctr, variable.names=c("beta0","beta1","sigma"),
n.iter=10000)
plot(results.B.ctr)
gelman.plot(results.B.ctr)
gelman.diag(results.B.ctr)
par(mfrow=c(1,3))
acf(results.B.ctr[[1]][,"beta0"],lag.max=30)
acf(results.B.ctr[[1]][,"beta1"],lag.max=30)
acf(results.B.ctr[[1]][,"sigma"],lag.max=30)
par(mfrow=c(1,1))
#effective sample size
cat("ESS for parameter beta0:", effectiveSize(results.B.ctr[[1]][,"beta0"]),"\n")
cat("ESS for parameter beta1:",effectiveSize(results.B.ctr[[1]][,"beta1"]),"\n")
cat("ESS for parameter sigma:",effectiveSize(results.B.ctr[[1]][,"sigma"]),"\n")
summary(results.B.ctr)
gelman.diag(results.B.ctr)
#1.18. Robustifying the model
Jump.ctr.robust.model <-
"model {
# prior
beta0 ~ dnorm(beta.mu.0,beta.tau.0)
beta1 ~ dnorm(beta.mu.0,beta.tau.0)
tau   ~ dgamma(tau.a,tau.b)
meanY <- mean(Year)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta1*(Year[i]-meanY)
Jump[i] ~ dt(mu[i],tau,df)
}
sigma <- 1/sqrt(tau)
}"
#Add degrees of freedom parameter to data list
Jump.robust.data <- list(n=n,Jump=Jump,Year=Year,beta.mu.0=0,beta.tau.0=0.001,tau.a=0.1,tau.b=0.1,df=3)
# Run JAGS to the completion of the "adaption" stage
results.A.robust <- jags.model(file=textConnection(Jump.ctr.robust.model),
data=Jump.robust.data, inits=Jump.inits,
n.chains=num.chains)
update(results.A.robust, n.iter=1000)
results.B.robust <- coda.samples(results.A.robust, variable.names=c("beta0","beta1","sigma"),
n.iter=10000)
plot(results.B.robust)
summary(results.B.robust)
# Aside: If add data for the last 2 Olympics---
# y.jump <- c(LongJumpOlympics$Gold,8.31,8.38)
# x.year <- c(LongJumpOlympics$Year,2012,2016)
# plot(y.jump ~ x.year,xlab="Year",ylab="Jump (m)",
#      type="n",main="Olympic Gold LongJump Distances (m)",cex=0.7)
# text(x.year,y.jump,x.year)
# see a decline in distances, not as linear either
#minutes
elapsed.time <- c(4,5,7,12,14,16,20,24,28,31,34,37.5,41)
#Fahrenheit
temperature <- c(126,125,123,120,119,118,116,115,114,113,112,111,110)
n <- length(elapsed.time)
# 2.1.  Plot Temperature vs Time
scatter.smooth(temperature ~ elapsed.time,xlab="Time",ylab="",main="Friction Experiment Data")
y <- -log((temperature-60)/70)
out <- lm(y ~ 0 + elapsed.time)
# (ii) Compute nonlinear least squares estimate
# estimate an initial value for theta
y <- -log((temperature-60)/70)
out <- lm(y ~ -1 + elapsed.time)
initial.theta <- coef(out)
cat("initial.theta:",initial.theta,"\n")
nl.1 <- nls(formula= temperature ~ 60 + 70*exp(-theta*elapsed.time),
start=list(theta=initial.theta))
cat("Estimated theta=",signif(coef(nl.1),4),"\n")
summary(nl.1)  # sigma.hat = 1.918
plot(temperature ~ elapsed.time,xlab="Time",ylab="",
main="Friction Experiment Data")
lines(elapsed.time,fitted(nl.1),col="red")
nl.2 <- nls(formula=temperature ~ beta0 + beta1*exp(-theta*elapsed.time),
start=list(beta0=50,beta1=50,theta=initial.theta))
coef(nl.2)
summary(nl.2)
scatter.smooth(temperature ~ elapsed.time,xlab="Time",ylab="",main="Friction Experiment Data")
lines(elapsed.time,fitted(nl.2),col="blue")
# (iv) Bayesian model
Friction.model <-
"model {
# data that will be read in are n,  temperature, elapsed.time and prior hyperparameters
# prior
theta ~ dexp(theta.0)
tau  ~ dgamma(tau.a,tau.b)
#Likelihood
for(i in 1:n) {
mu[i]  <- 60+70*exp(-theta*elapsed.time[i])
temperature[i] ~ dnorm(mu[i],tau)
}
sigma <- 1/sqrt(tau)
}"
num.chains <- 3
friction.data <- list(temperature=temperature,elapsed.time=elapsed.time,n=n,theta.0=100,tau.a=0.01,tau.b=0.01)
friction.inits <- list(list(theta=0.1,tau=0.1),
list(theta=0.2,tau=0.1),
list(theta=0.2,tau=0.2))
results.friction.A<- jags.model(file=textConnection(Friction.model),
data=friction.data, inits=friction.inits,
n.chains=num.chains)
update(results.friction.A, n.iter=1000)
results.friction.B <- coda.samples(results.friction.A,
variable.names=c("theta","sigma"), n.iter=10000)
plot(results.friction.B)
summary(results.friction.B)
# Now fit all three parameters in the model
Friction.3par.model <-
"model {
# data that will be read in are n,  temperature, elapsed.time and prior hyperparameters
# prior
theta ~ dexp(theta.0)
beta0 ~ dexp(beta0.0)
beta1 ~ dexp(beta1.0)
tau  ~ dgamma(tau.a,tau.b)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta1*exp(-theta*elapsed.time[i])
temperature[i] ~ dnorm(mu[i],tau)
}
sigma <- 1/sqrt(tau)
}"
friction.3par.data <- list(temperature=temperature,elapsed.time=elapsed.time,n=n,theta.0=100,beta0.0=1/60,beta1.0=1./70,tau.a=0.01,tau.b=0.01)
friction.3par.inits <- list(list(theta=0.1,beta0=1, beta1=10,tau=0.1),
list(theta=0.2,beta0=50,beta1=80,tau=0.1),
list(theta=0.2,beta0=100,beta1=100,tau=0.2))
results.friction.3par.A<- jags.model(file=textConnection(Friction.3par.model),
data=friction.3par.data, inits=friction.3par.inits,
n.chains=num.chains)
update(results.friction.3par.A, n.iter=1000)
results.friction.3par.B <- coda.samples(results.friction.3par.A,
variable.names=c("theta","beta0","beta1","sigma"), n.iter=50000)
par(mar=c(2,2,2,2))
#Setting margins
plot(results.friction.3par.B)
summary(results.friction.3par.B)
library(LearnBayes)
data(birdextinct)
n   <- nrow(birdextinct)
extinct.time <- birdextinct$time
avg.no.nests <- birdextinct$nesting
size.ind  <- birdextinct$size   # 0 = large, 1= small
mig.ind   <- birdextinct$status # 0 = mig, 1=resident
size                             <- rep("Small",n)
size[birdextinct$size==0]        <- "Large"
migratory                        <- rep("Resident",n)
migratory[birdextinct$status==0] <- "Migratory"
hist(extinct.time,xlab="",main="Exinction Times (years)")
log.extinct <- log(extinct.time)
par(mfrow=c(2,2),oma=c(0,0,3,0))
par(mar=c(2,2,2,2))
#Setting margins
hist(log.extinct,xlab="",main="Log(Extinction)")
scatter.smooth(log.extinct ~ avg.no.nests,xlab="Avg #Nests",ylab="",
main="vs Abundance")
boxplot(split(log.extinct,size),main="vs Size ")
boxplot(split(log.extinct,migratory),main="vs Residency")
mtext("Log Time till Extinction",outer=TRUE)
par(mfrow=c(1,1))
extinct.mlr <- lm(log.extinct ~ avg.no.nests + size + migratory)
summary(extinct.mlr)
# Could also centre the average number of nests variable before doing the regression.
nest.ctr <- avg.no.nests-mean(avg.no.nests)
extinct.ctr.mlr <- lm(log.extinct~ nest.ctr+size+migratory)
summary(extinct.ctr.mlr)
par(mfrow=c(2,2))
plot(extinct.mlr)
par(mfrow=c(1,1))
# 3.6. Bayesian analysis
#We have computed the centered version of avg.no.nests in the solution to 2.1, and stored it in variable nest.ctr
extinct.model <-
"model {
# data that will be read in are n, log.extinct, nest.ctr, size.ind, mig.ind plus prior hyperparameters
# prior
beta0     ~ dnorm(mu.0,tau.0)
beta.nest ~ dnorm(mu.0,tau.0)
beta.size ~ dnorm(mu.0,tau.0)
beta.mig  ~ dnorm(mu.0,tau.0)
tau       ~ dgamma(tau.a,tau.b)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta.nest*nest.ctr[i] + beta.size*size.ind[i] + beta.mig*mig.ind[i]
log.extinct[i] ~ dnorm(mu[i],tau)
}
sigma2 <- 1/tau
}"
extinct.data <- list(log.extinct=log.extinct,nest.ctr=nest.ctr,size.ind=size.ind,
mig.ind=mig.ind, n=n, mu.0=0, tau.0=1.0e-5, tau.a=0.01, tau.b=0.01)
#Specify three sets of intiial conditions
extinct.inits <- list(list(beta0=0.1,beta.nest=1, beta.size=10,beta.mig=3,tau=0.1),
list(beta0=0.2,beta.nest=-5,beta.size=5, beta.mig=1,tau=0.1),
list(beta0=0.2,beta.nest=10,beta.size=20,beta.mig=-2,tau=0.2))
num.chains <- 3
results.extinct.A<- jags.model(file=textConnection(extinct.model),
data=extinct.data , inits=extinct.inits ,
n.chains=num.chains)
#Can also run without specifying initial values for the three chains.
#results.extinct.A<- jags.model(file=textConnection(extinct.model),
#                                  data=extinct.data,
#                                  n.chains=num.chains)
update(results.extinct.A, n.iter=1000)
results.extinct.B <- coda.samples(results.extinct.A,
variable.names=c("beta0","beta.nest", "beta.size","beta.mig","sigma2"),
n.iter=10000)
#Produce trace plots
par(mfrow=c(3,2))
plot(results.extinct.B[,c("beta.nest", "beta.size","beta.mig")])
par(mfrow=c(2,2))
plot(results.extinct.B[,c("beta0","sigma2")])
par(mfrow=c(1,1))
par(mfrow=c(3,2))
gelman.plot(results.extinct.B)
gelman.diag(results.extinct.B)
par(mfrow=c(3,2))
#Setting 3x2 plots displayed next to each other
par(mar=c(1,1,1,1))
#Setting the margins to be small
acf(results.extinct.B[[1]][,"beta0"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.nest"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.size"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.mig"],lag.max=30)
acf(results.extinct.B[[1]][,"sigma2"],lag.max=30)
par(mfrow=c(1,1))
#Changing back to the 1 plot being displayed at a time
cat("ESS for beta0:",effectiveSize(results.extinct.B[[1]][,"beta0"]),"\n")
cat("ESS for beta.nest:",effectiveSize(results.extinct.B[[1]][,"beta.nest"]),"\n")
cat("ESS for beta.size:",effectiveSize(results.extinct.B[[1]][,"beta.size"]),"\n")
cat("ESS for beta.mig:",effectiveSize(results.extinct.B[[1]][,"beta.mig"]),"\n")
cat("ESS for sigma2:",effectiveSize(results.extinct.B[[1]][,"sigma2"]),"\n")
#Get summary statistics for posterior distributions
summary(results.extinct.B)
#Posterior predictive checks.
#First compute studentised residuals fitted values
resmat=as.matrix(results.extinct.B)
dim(resmat)
niterf=nrow(resmat)
beta0=resmat[,4]; beta1=resmat[,2] ;beta2=resmat[,3]; beta3=resmat[,1]; sigma2=resmat[,5]
x=cbind(rep(1,n),nest.ctr,size.ind,mig.ind)
H=x%*%solve((t(x)%*%x))%*%t(x)
fittedvalues=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
fittedvalues[,l]=beta0[l]*x[,1]+beta1[l]*x[,2]+beta2[l]*x[,3]+beta3[l]*x[,4]
}
#studentised residuals
studentisedresid=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
for(i in 1:n){
studentisedresid[i,l]=(log.extinct[i]-fittedvalues[i,l])/(sqrt(sigma2[l]*(1-diag(H)[i])))
}
}
#posterior mean of studentised residuals
studentisedresidm=numeric(n)
for(i in 1:n){
studentisedresidm[i]=mean(studentisedresid[i,])
}
#Plot of posterior mean studentised residual versus observation number.
par(mfrow=c(1,1))
plot(seq_along(studentisedresidm),studentisedresidm,xlab="Index",ylab="Bayesian studentised residual",ylim=c(-3,3))
#QQ-plot
qqnorm(studentisedresidm,xlim=c(-3,3),ylim=c(-3,3),lwd=2)
qqline(studentisedresidm,col=2,lwd=2)
#Compute posterior mean fitted values
fittedvaluesm=numeric(n)
for(i in 1:n){
fittedvaluesm[i]=mean(fittedvalues[i,])
}
plot(fittedvaluesm,studentisedresidm,xlab="Fitted value (posterior mean)",ylab="Bayesian Studentised residual (posterior mean)")
#Now do some predictive checks
#First replicate the data
yrep=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
for(i in 1:n){
yrep[i,l]=rnorm(1,beta0[l]*x[i,1]+beta1[l]*x[i,2]+beta2[l]*x[i,3]+beta3[l]*x[i,4],sigma2[l])
}
}
#Compute posterior preditive distribution of min amd max log(extinction time).
yrepmin=apply(yrep,2,min)
yrepmax=apply(yrep,2,max)
par(mfrow=c(2,1))
hist(yrepmin,col="gray40",main="Predictive distribution for minimum")
abline(v=min(log.extinct),col="red",lwd=2)
hist(yrepmax,col="gray40",main="Predictive distribution for maximum")
abline(v=max(log.extinct),col="red",lwd=2)
def.extinct.model.with.replicates <-
"model {
# data that will be read in are n, log.extinct, nest.ctr, size.ind, mig.ind plus prior hyperparameters
# prior
beta0     ~ dnorm(mu.0,tau.0)
beta.nest ~ dnorm(mu.0,tau.0)
beta.size ~ dnorm(mu.0,tau.0)
beta.mig  ~ dnorm(mu.0,tau.0)
tau       ~ dgamma(tau.a,tau.b)
#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta.nest*nest.ctr[i] + beta.size*size.ind[i] + beta.mig*mig.ind[i]
log.extinct[i] ~ dnorm(mu[i],tau)
log.extinct.replicate[i]~ dnorm(mu[i],tau)
}
sigma2 <- 1/tau
}"
extinct.data <- list(log.extinct=log.extinct,nest.ctr=nest.ctr,size.ind=size.ind,
mig.ind=mig.ind, n=n, mu.0=0, tau.0=1.0e-5, tau.a=0.01, tau.b=0.01)
num.chains <- 3
model.extinct.with.replicates<- jags.model(file=textConnection(def.extinct.model.with.replicates),
data=extinct.data ,
n.chains=num.chains)
#Can also run without specifying initial values for the three chains.
#results.extinct.A<- jags.model(file=textConnection(extinct.model),
#                                  data=extinct.data,
#                                  n.chains=num.chains)
update(model.extinct.with.replicates, n.iter=1000)
results.extinct.with.replicates <- coda.samples(model.extinct.with.replicates,
variable.names=c("log.extinct.replicate"),
n.iter=10000)
#We only save the replicates, and not the other variables "beta0","beta.nest", "beta.size","beta.mig","sigma2"
#Now do some predictive checks using the replicates from JAGS
#First, we convert the output into a matrix
matrix.results.extinct.with.replicates=as.matrix(results.extinct.with.replicates)
#The yrep matrix is easily obtained from this,
#we need to use transpose t() due to the way we use yrep in the plots
yrep=t(matrix.results.extinct.with.replicates[1:niterf,1:n])
#Compute posterior preditive distribution of min amd max log(extinction time).
yrepmin=apply(yrep,2,min)
yrepmax=apply(yrep,2,max)
par(mfrow=c(2,1))
hist(yrepmin,col="gray40",main="Predictive distribution for minimum")
abline(v=min(log.extinct),col="red",lwd=2)
hist(yrepmax,col="gray40",main="Predictive distribution for maximum")
abline(v=max(log.extinct),col="red",lwd=2)
yrep <- matrix(0, nrow=n, ncol=nsamp)
for (i in 1: nsamp) {
yrep[, i] <- rnorm(n, mean=fittedvalues[, i], sd=sigma[i])
}
yrepmin <- apply(yrep, 1, min)
yrepmax <- apply(yrep, 1, max)
par(mfrow=c(2, 2))
par(mar=c(2, 2, 2, 2))
hist(yrepmin, col="grey40")
abline(v=min(log.extinct), col="red")
hist(yrepmax, col="grey40")
abline(v=max(log.extinct), col="red")
yrep <- matrix(0, nrow=n, ncol=nsamp)
for (i in 1: nsamp) {
yrep[, i] <- rnorm(n, mean=fittedvalues[, i], sd=sigma[i])
}
yrepmin <- apply(yrep, 1, min)
yrepmax <- apply(yrep, 1, max)
par(mfrow=c(2, 1))
par(mar=c(2, 2, 2, 2))
hist(yrepmin, col="grey40")
abline(v=min(log.extinct), col="red")
hist(yrepmax, col="grey40")
abline(v=max(log.extinct), col="red")
#Now do some predictive checks
#First replicate the data
yrep=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
for(i in 1:n){
yrep[i,l]=rnorm(1,beta0[l]*x[i,1]+beta1[l]*x[i,2]+beta2[l]*x[i,3]+beta3[l]*x[i,4],sigma2[l])
}
}
#Compute posterior preditive distribution of min amd max log(extinction time).
yrepmin=apply(yrep,2,min)
yrepmax=apply(yrep,2,max)
par(mfrow=c(2,1))
hist(yrepmin,col="gray40",main="Predictive distribution for minimum")
abline(v=min(log.extinct),col="red",lwd=2)
hist(yrepmax,col="gray40",main="Predictive distribution for maximum")
abline(v=max(log.extinct),col="red",lwd=2)
yrep <- matrix(0, nrow=n, ncol=nsamp)
for (i in 1: nsamp) {
yrep[, i] <- rnorm(n, mean=fittedvalues[, i], sd=sigma[i])
}
yrepmin <- apply(yrep, 1, min)
yrepmax <- apply(yrep, 1, max)
par(mfrow=c(2, 1))
hist(yrepmin, col="grey40")
abline(v=min(log.extinct), col="red")
hist(yrepmax, col="grey40")
abline(v=max(log.extinct), col="red")
