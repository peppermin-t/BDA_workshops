**University of Edinburgh**

**School of Mathematics**

**Bayesian Data Analysis, 2023/2024, Semester 2**

**Workshop 2: Introduction to JAGS**

**Note**: Before starting this practical, you might want to spend some time looking at the JAGS examples we have discussed during Lecture 3. If you already did it, then go directly to question 1. The code below loads JAGS.

```{r}
library(rjags)
#If it ran correctly, you should see 
#Loading required package: coda
#Linked to JAGS 4.3.1
#Loaded modules: basemod,bugs
```

**1. Simple linear regression with robustification**

**Winning Olympic Men's Long Jump Distances (adapted from Witmer, 2017)**

**The data are the winning men's long jump distances (m) from 1900 through 2008. You will fit a linear regression of the distances as a function of Olympic year:** $$\begin{aligned}
Jump & = & \beta_0 + \beta_1 \mbox{Year} + \epsilon\end{aligned}$$ **three different ways: standard frequentist approach, a Bayesian approach assuming normal errors, and a Bayesian approach assuming a** $t$ **distribution for errors.**

**Run the following commands in `R` to begin (this will install and load the package Stat2Data and load the Long Jump dataset).**

```{r}
library(Stat2Data)
#If not available, please use install.packages("Stat2Data")
data("LongJumpOlympics")   #Makes the dataset available in this R session
Jump <- LongJumpOlympics$Gold
Year <- LongJumpOlympics$Year
#You can get more info about the dataset by the command
#help(topic="LongJumpOlympics",package="Stat2Data")
```

**1.1. We start by carrying out some exploratory data analysis. Plot Jump vs Year. What does the relationship look like?**

The plot is shown below. The relationship looks reasonably linear.

```{r}
#1.1. Plot Jump vs Year 
par(mfrow=c(1,1))
plot(Jump ~ Year,xlab="Year",ylab="Jump (m)",
     type="n",main="Olympic Gold LongJump Distances (m)",cex=0.6)
text(x=Year,y=Jump,labels=Year)
```

**1.2. Fit a simple linear regression on Jump against Year using the `lm` function, and make a plot of the data with the fitted line overlaid using the `abline` function.**

See the `R` code below fitting the linear model. The figue shows the data with the best fit line overlaid.

```{r}
#1.2. Fit simple linear regression model.
m1 <- lm(Jump ~ Year)
summary(m1)
#Plot data with fit overlaid.
par(mfrow=c(1,1))
plot(Jump ~ Year,xlab="Year",ylab="Jump (m)",
     type="n",main="Olympic Gold LongJump Distances (m)",cex=0.6)
text(x=Year,y=Jump,labels=Year)
abline(m1,col="blue")
```

**1.3. Based on this model, every 4 years we would expect the jump distance to change by what amount?**

The coefficient $\beta_1 = 0.014066$ is the predicted increase in the length of the jump per year, and so the predicted increase every four years is $0.05626$m.

```{r}
#1.3. Compute change in winning distance every 4 years
cat("Predicted change every 4 years=",coef(m1)["Year"]*4,"meters \n")
```

**1.4. Plot the residuals against Year (using the `residuals` function). One year stands out, which one is it?**

The residual plot is shown in the figure below. 1968 is a clear outlier.

```{r}
#1.4. Plot residuals against Year
plot(residuals(m1) ~ Year,xlab="Year",ylab="Residuals")
abline(h=0,col="red")
```

**1.5. For a more detailed residual analysis, type `par(mfrow=c(2,2))`, and use the `plot` function operating on the `lm` object (you'll see 4 plots).**

The full set of diagnostic plots are shown in the figures below. These support the conclusion that 1968 is an outlier.

```{r}
par(mfrow=c(2,2)); plot(m1); par(mfrow=c(1,1))
```

```{r}
plot(m1,which=3)
```

**1.6. Remove the outlier from the data set and refit the model, then recompute the above residual diagnostics. What do you observe?**

Diagnostic plots after removing the date point for 1968 are shown in the figures below. These look much better. The summary of the revised fit is also included.

```{r}
#1.6. Remove outlier and refit
omit <- Year==1968
m2 <- lm(Jump[!omit] ~ Year[!omit])
par(mfrow=c(2,2))
plot(m2)
par(mfrow=c(1,1))
```

```{r}
summary(m2)
```

**1.7. Carry out a Bayesian linear regression analysis using `rjags`. As in the frequentist case assume** $\epsilon$$\sim$ **Normal(0,** $\sigma^2$**). Use the following priors for the three parameters:** $$\begin{aligned}
\beta_0, \beta_1 & \sim & \mbox{Normal} \left ( \mu_0=0, \tau_0=0.001 \right ) \\
\tau & \sim & \mbox{Gamma} \left ( a=0.1, b=0.1 \right )
\end{aligned}$$

**Write the *model* statement, which includes the likelihood calculation and the prior distribution. Include a calculation of** $\sigma = 1/\sqrt{\tau}$.

```{r}
#1.7. Create model block for JAGS
Jump.model <-   
  "model {
  # data that will be read in are n,  Jump, Year and prior hyperparameters
  # prior
  beta0 ~ dnorm(beta.mu.0,beta.tau.0)
  beta1 ~ dnorm(beta.mu.0,beta.tau.0)
  tau   ~ dgamma(tau.a,tau.b)

  #Likelihood
  for(i in 1:n) {
    mu[i]  <- beta0+beta1*Year[i]
    Jump[i] ~ dnorm(mu[i],tau)
  }

  sigma <- 1/sqrt(tau)
 
}"
```

**1.8. Create an `R` object for the data, which includes Jump, Year,** $n$**=26 and the values of the prior hyperparameters** $\mu_0$**,** $\tau_0$**,** $a$ **and** $b$**.**

```{r}
# 2.8. Create data input for JAGS
n     <- length(Year) 
Jump.data <- list(n=n,Jump=Jump,Year=Year,beta.mu.0=0,beta.tau.0=0.001,tau.a=0.1,tau.b=0.1)

```

**1.9. Create an `R` object for 3 sets of initial values; such as\
`my.inits <- list(c(b0=0.1,b1=0.2,tau=0.1), c(b0=-1,b1=3,tau=0.3),c(b0=1,b1=0,tau=.8))`**

```{r}
# 2.9. Create initial values for JAGS

num.chains <- 3
Jump.inits <- list(list(beta0=0.2, beta1=0.1, tau=0.1),
                   list(beta0=4.0, beta1=0.0, tau=0.01),
                   list(beta0=-3.0,beta1=-2.0,tau=0.001))
```

**1.10. Execute `jags.model` using the above objects. Note `n.chains` should be set equal to 3. How many unobserved stochastic nodes were there? How many observed?**

```{r}
# 2.10. Execute JAGS model
# Run JAGS to the completion of the "adaption" stage 
results.A <- jags.model(file=textConnection(Jump.model), 
                        data=Jump.data, inits=Jump.inits, 
                        n.chains=num.chains)
```

There are 26 observed stochastic nodes, corresponding to the 26 data points, and 3 unobserved stochastic nodes, corresponding to the unknown model parameters ($\beta_0$, $\beta_1$, $\sigma$).

**1.11. Use `update` to carry out an initial MCMC run (burn-in) with 1,000 iterations.**

```{r}
# 2.11. Run burn-in
update(results.A, n.iter=1000)
```

**1.12. Now make a longer MCMC run using the `coda.samples` function with 10,000 iterations and have the results for** $\beta_0$**,** $\beta_1$**, and** $\sigma$ **returned.**

```{r}
# 2.12. Get production samples

results.B <- coda.samples(results.A, variable.names=c("beta0","beta1","sigma"),n.iter=10000)
```

**1.13. Plot the results from `coda.samples`. These are the trace plots. Do you think that the chains have converged for each of the 3 parameters?**

The trace plots are shown in the figures below. These show very slow convergence --- the three separate runs have only just begun to converge at the end of the run.

```{r}
# 2.13. Plot the results to check for convergence
# Check for convergence before looking at posterior dist'n summaries
par(mfrow=c(1,1))
plot(results.B) 
```

**1.14. You may have noticed from the trace plots that** $\mathbf{\beta_0}$ **and** $\mathbf{\beta_1}$ **are mixing slowly. That's indicative of significant autocorrelation. Use the `acf` function to see how much correlation there is. For example, if the results from `coda.samples` are called `res`, for a parameter named beta0: `acf(res[[1]][,"beta0"],lag.max=100)`**

The autocorrelation functions are shown in the figures below. The autocorrelation is still very high, even at lag 100.

```{r}
#1.14. Evaluate autocorrelation functions
par(mfrow=c(1,3))
acf(results.B[[1]][,"beta0"],lag.max=100)
acf(results.B[[1]][,"beta1"],lag.max=100)
acf(results.B[[1]][,"sigma"],lag.max=100)
```

**1.15. Also take a look at the effective sample sizes per parameter, e.g.,`effectiveSize(res[[1]][,"beta0"])`**

The 'R' script below computes the effective sample sizes. These are very small, corresponding to poor mixing.

```{r}
#1.15. Evaluate effective sample sizes for each parameter.
effectiveSize(results.B[[1]][,"beta0"])
effectiveSize(results.B[[1]][,"beta1"])
effectiveSize(results.B[[1]][,"sigma"])
```

**1.16. In Lecture 1, the Gelman-Rubin (Brooks-Gelman-Rubin) statistic was discussed. This is a quantitative measure of apparent convergence that is based upon the degree of overlap of 2 or more chains after each iteration. The BGR statistic roughly approximates the ratio of the variability between chains to the variability within chains (like an F statistic in ANOVA). The general idea of the statistic is that the the ratio of those two measures should be around 1 at convergence, thus BGR=1 is "good". Use the `coda` package function called `gelman.plot` to plot the BGR statistic for each of the parameters against the MCMC iteration. And use `gelman.diag` for numerical summaries. What do you think about convergence now?**

The Gelman-Rubin statistics and plots are shown in the figures below. These show that the BGR is much bigger than $1$ and so the chains have not yet converged, as the trace plots suggested.

```{r}
# 2.16. Compute the Gelman-Rubin statistic
par(mfrow=c(2,2))
gelman.plot(results.B)
```

```{r}
gelman.diag(results.B)
```

These show that the BGR is much bigger than 1 and so the chains have not yet converged, as the trace plots suggested.

**1.17. Centring the covariate, in this case Year, sometimes helps convergence. Modify your Model statement slightly by creating a new variable `meanY`, and then subtract that from the `Year[i]` values in the for loop. Repeat the above steps. How does convergence now look? Use the `summary` function on the JAGS output to examine the posterior means and standard deviations for** $\beta_0$, $\beta_1$, **and** $\sigma$. **How do the posterior mean for** $\beta_1$ **and** $\sigma$ **compare to the maximum likelihood estimates obtained in 2.2?**

The trace plots, autocorrelation functions and Gelman-Rubin statistics for the regression after centring are shown in the figures below. These show much better convergence than the previous results.

```{r}
#1.17. Work with centered covariate to see if that helps----
Jump.ctr.model <-   
  "model {
  # prior
  beta0 ~ dnorm(beta.mu.0,beta.tau.0)
  beta1 ~ dnorm(beta.mu.0,beta.tau.0)
  tau   ~ dgamma(tau.a,tau.b)
  meanY <- mean(Year)

  #Likelihood
  for(i in 1:n) {
    mu[i]  <- beta0+beta1*(Year[i]-meanY)
    Jump[i] ~ dnorm(mu[i],tau)
  }

  sigma <- 1/sqrt(tau)
 
}"

# Run JAGS to the completion of the "adaption" stage 
results.A.ctr <- jags.model(file=textConnection(Jump.ctr.model), 
                        data=Jump.data, inits=Jump.inits, 
                        n.chains=num.chains)
update(results.A.ctr, n.iter=1000)
results.B.ctr <- coda.samples(results.A.ctr, variable.names=c("beta0","beta1","sigma"),
                              n.iter=10000)

plot(results.B.ctr) 
gelman.plot(results.B.ctr)
gelman.diag(results.B.ctr)


par(mfrow=c(1,3))
acf(results.B.ctr[[1]][,"beta0"],lag.max=30)
acf(results.B.ctr[[1]][,"beta1"],lag.max=30)
acf(results.B.ctr[[1]][,"sigma"],lag.max=30)


par(mfrow=c(1,1))

#effective sample size
cat("ESS for parameter beta0:", effectiveSize(results.B.ctr[[1]][,"beta0"]),"\n")
cat("ESS for parameter beta1:",effectiveSize(results.B.ctr[[1]][,"beta1"]),"\n")
cat("ESS for parameter sigma:",effectiveSize(results.B.ctr[[1]][,"sigma"]),"\n")

summary(results.B.ctr)

```

```{r}
gelman.diag(results.B.ctr)

```

**1.18. *Robustifying the regression.* As was noted in Lecture 3, the effects of extreme observations or "outliers" on regressing results can be diminished by using a** $t$ **distribution for the observations. For simplicity, assume a** $t$ **distribution with 3 df for the distribution of errors. Revise the JAGS model code accordingly (continuing to work with the centred covariate) and re-run. Recall from Lecture 3 that the necessary change to the code is to replace `dnorm` with `dt` and add an additional argument to `data` for the df (=3). How did the posterior mean of** $\beta_1$ **change? Compare it to the estimate in 1.2 when the extreme observation is removed.**

The figure below shows the trace plots for a Bayesian robust regression fit to the longjump data.

```{r}
#1.18. Robustifying the model
Jump.ctr.robust.model <-   
  "model {
  # prior
  beta0 ~ dnorm(beta.mu.0,beta.tau.0)
  beta1 ~ dnorm(beta.mu.0,beta.tau.0)
  tau   ~ dgamma(tau.a,tau.b)
  meanY <- mean(Year)

  #Likelihood
  for(i in 1:n) {
    mu[i]  <- beta0+beta1*(Year[i]-meanY)
    Jump[i] ~ dt(mu[i],tau,df)
  }

  sigma <- 1/sqrt(tau)
 
}"
#Add degrees of freedom parameter to data list
Jump.robust.data <- list(n=n,Jump=Jump,Year=Year,beta.mu.0=0,beta.tau.0=0.001,tau.a=0.1,tau.b=0.1,df=3)

# Run JAGS to the completion of the "adaption" stage 
results.A.robust <- jags.model(file=textConnection(Jump.ctr.robust.model), 
                        data=Jump.robust.data, inits=Jump.inits, 
                        n.chains=num.chains)
update(results.A.robust, n.iter=1000)
results.B.robust <- coda.samples(results.A.robust, variable.names=c("beta0","beta1","sigma"),
                              n.iter=10000)
plot(results.B.robust)

summary(results.B.robust)

# Aside: If add data for the last 2 Olympics--- 
# y.jump <- c(LongJumpOlympics$Gold,8.31,8.38)
# x.year <- c(LongJumpOlympics$Year,2012,2016)
# plot(y.jump ~ x.year,xlab="Year",ylab="Jump (m)",
#      type="n",main="Olympic Gold LongJump Distances (m)",cex=0.7)
# text(x.year,y.jump,x.year)
# see a decline in distances, not as linear either

```

The new posterior mean estimate of $\beta_1$ is $0.0139$, which compares well to the value derived from the least squares fit after removing the outlier ($0.0137$). The posterior mean estimate of $\sigma$ ($0.208$) is somewhat larger than the least squares estimate ($0.191$), but the meaning of the parameter is now somewhat different.

**2. Nonlinear Regression. Newton's law of cooling, from Bates and Watts (2008)**

**The following data are measurements over a 41 minute period of the temperature of a bore after that bore had been rubbed inside "a stationary cylinder and pressed against the bottom by means of a screw". The bore was turned by a team of horses (this is an experiment with friction from 1798 by a Count Rumford).**

```{r}
 #minutes
 elapsed.time <- c(4,5,7,12,14,16,20,24,28,31,34,37.5,41) 
 #Fahrenheit
 temperature <- c(126,125,123,120,119,118,116,115,114,113,112,111,110) 
 n <- length(elapsed.time)

```

**An underlying theoretical model based on Newton's law of cooling suggests that temperature should decline over time according to the following model.** $$\begin{aligned}
\mbox{temperature} & = & 60 +70e^{-\theta \;\mbox{elapsed.time}}
\end{aligned}$$ You are to evaluate this model, i.e., make estimates of $\theta$ using classical and Bayesian techniques.

**2.1. Plot temperature against time (use the `scatter.smooth` function to draw a nonparametric regression line through the points).**

The plot of temperature versus elapsed time is shown in the figure below.

```{r}
# 2.1.  Plot Temperature vs Time
scatter.smooth(temperature ~ elapsed.time,xlab="Time",ylab="",main="Friction Experiment Data")
```

**2.2. Fit the model in (i) using a classical approach that assumes that observations have model errors are iid Normal(0,**$\sigma^2$**).** $$\begin{aligned}
 \mbox{temperature} & \sim & \mbox{Normal} \left ( 60 +70e^{-\theta \; \mbox{elapsed.time}}, \sigma^2 \right )
\end{aligned}$$ **Use the `nls` function in R. The format of `nls` in this case: `nl.1 <- nls(formula= temperature ~ 60 + 70*exp(-theta*elapsed.time), start=list(theta=initial.theta))` where `initial.theta` is an initial guess as to what** $\theta$ **is.**

**One way to get an estimate of** $\theta$ **is to "linearize" Newton's law of cooling as follows:** $$\begin{aligned}
-\ln \left ( \frac{(\mbox{temperature}-60)}{70} \right )  & = &  \theta*\mbox{elapsed.time}
\end{aligned}$$ **and then fit the resulting linear model with the `lm` function.**

```{r}
y <- -log((temperature-60)/70)
out <- lm(y ~ 0 + elapsed.time)  
```

**Use the estimated coefficient in `out` as the value of `initial.theta`. After fitting the model, plot the fit and the observations.**

Please see the `R` script below where this procedure was implemented, the value of initial.theta and the final fit is printed out, as well as a summary of the nonlinear least squares fit.

```{r}
# (ii) Compute nonlinear least squares estimate
# estimate an initial value for theta
y <- -log((temperature-60)/70)
out <- lm(y ~ -1 + elapsed.time)
initial.theta <- coef(out)
cat("initial.theta:",initial.theta,"\n")

nl.1 <- nls(formula= temperature ~ 60 + 70*exp(-theta*elapsed.time),
             start=list(theta=initial.theta))
cat("Estimated theta=",signif(coef(nl.1),4),"\n")

summary(nl.1)  # sigma.hat = 1.918
```

The plot below shows the fitted curve along with the data.

```{r}
plot(temperature ~ elapsed.time,xlab="Time",ylab="",
main="Friction Experiment Data")
lines(elapsed.time,fitted(nl.1),col="red")
```

**How does the fit look?**

The fit is clearly not a good representation of the observations.

**2.3. Instead of using 60 and 70 in Newton's law of cooling as known values, refit the model estimating the coefficients.**

Fitting the additional coefficients as well we obtain the following results.

```{r}
nl.2 <- nls(formula=temperature ~ beta0 + beta1*exp(-theta*elapsed.time),
            start=list(beta0=50,beta1=50,theta=initial.theta))
coef(nl.2)
summary(nl.2)
scatter.smooth(temperature ~ elapsed.time,xlab="Time",ylab="",main="Friction Experiment Data")
lines(elapsed.time,fitted(nl.2),col="blue") 
```

**Compare the estimated coefficients to the assumed values and plot the fitted line over the top of the previous plot. Has the fit improved?**

The best-fit coefficients, $106.2$ and $22.9$, differ considerably from the assumed values of $60$ and $70$. The residual standard error is now much smaller and the fit matches the data much better, as shown in the figure above.

**2.4. Use JAGS to fit two Bayesian nonlinear regression models: one based on Newton's law of cooling, as in (ii), and another where all three coefficients are estimated, as in (iii). Assume that temperatures are normally distributed in the likelihood model.**

**In both cases use exponential distributions for the priors for** $\theta$ **and then for** $\beta_0$ **and** $\beta_1$ **(to ensure that the posterior distributions are positive valued). To pick the exponential distribution hyperparameter, say** $\alpha$**, note that if** $X \sim$**Exp**$(\alpha)$**,** $\mathbb{E}[X]$ **=** $1/\alpha$**. Pick a large value for the hyperparameter for** $\theta$ **such that the expected value of** $\theta$ **is less than 1. For the 2nd model (as in (iii)), select hyperparameter values for** $\beta_0$ **and** $\beta_1$ **such that the expected values are 60 and 70, respectively. Note: in JAGS, the exponential density is written `theta ~ dexp(a)` given hyperparameter** $a$**.**

**Compare the posterior means for** $\theta$ **in both models with the frequentist estimates.\
Likewise compare the posterior means for** $\beta_0$ **and** $\beta_1$ **for the second model.**

First, we fit the Bayesian model with $\beta_0$ and $\beta_1$ fixed at 60 and 70, respectively.

```{r}
# (iv) Bayesian model 
Friction.model <-   
  "model {
  # data that will be read in are n,  temperature, elapsed.time and prior hyperparameters
  # prior
  theta ~ dexp(theta.0)
  tau  ~ dgamma(tau.a,tau.b)
 
  #Likelihood
  for(i in 1:n) {
    mu[i]  <- 60+70*exp(-theta*elapsed.time[i])
    temperature[i] ~ dnorm(mu[i],tau)
  }

  sigma <- 1/sqrt(tau)
}"

num.chains <- 3

friction.data <- list(temperature=temperature,elapsed.time=elapsed.time,n=n,theta.0=100,tau.a=0.01,tau.b=0.01)
friction.inits <- list(list(theta=0.1,tau=0.1),
                       list(theta=0.2,tau=0.1),
                       list(theta=0.2,tau=0.2))
results.friction.A<- jags.model(file=textConnection(Friction.model), 
                               data=friction.data, inits=friction.inits, 
                               n.chains=num.chains)
update(results.friction.A, n.iter=1000)
results.friction.B <- coda.samples(results.friction.A, 
          variable.names=c("theta","sigma"), n.iter=10000)
plot(results.friction.B)

summary(results.friction.B)  
 
```

The trace plots show that the chains were mixing well and have converged. The posterior mean estimate of $\theta$ is $0.0094$, which agrees with the estimate obtained in question 3.2.

Now we fit the second model, where parameters $\beta_0$ and $\beta_1$ are also allowed to vary.

```{r}
# Now fit all three parameters in the model
Friction.3par.model <-   
  "model {
# data that will be read in are n,  temperature, elapsed.time and prior hyperparameters
# prior
theta ~ dexp(theta.0)
beta0 ~ dexp(beta0.0)
beta1 ~ dexp(beta1.0)
tau  ~ dgamma(tau.a,tau.b)

#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta1*exp(-theta*elapsed.time[i])
temperature[i] ~ dnorm(mu[i],tau)
}

sigma <- 1/sqrt(tau)

}"

friction.3par.data <- list(temperature=temperature,elapsed.time=elapsed.time,n=n,theta.0=100,beta0.0=1/60,beta1.0=1./70,tau.a=0.01,tau.b=0.01)
friction.3par.inits <- list(list(theta=0.1,beta0=1, beta1=10,tau=0.1),
                       list(theta=0.2,beta0=50,beta1=80,tau=0.1),
                       list(theta=0.2,beta0=100,beta1=100,tau=0.2))
results.friction.3par.A<- jags.model(file=textConnection(Friction.3par.model), 
                               data=friction.3par.data, inits=friction.3par.inits, 
                               n.chains=num.chains)
update(results.friction.3par.A, n.iter=1000)
results.friction.3par.B <- coda.samples(results.friction.3par.A, 
          variable.names=c("theta","beta0","beta1","sigma"), n.iter=50000)
par(mar=c(2,2,2,2))
#Setting margins
plot(results.friction.3par.B)
summary(results.friction.3par.B)
```

Again, the trace plots show good convergence. The posterior mean estimates of $\theta$ ($0.0388$), $\beta_0$ ($105.4$) and $\beta_1$ ($23.5$) agree within their uncertainties with those obtained from the non-linear least squares fit, which were $0.0409$, $106.2$ and $22.9$ respectively.

**3. Multiple Linear Regression\
Factors Affecting Extinction Times of 62 Land Bird Species, adapted from Albert, 2009\
The data are taken from Ramsey and Schafer (1997), who took them from Pimm et al. 1988, and are available in the `LearnBayes` package as the object `birdextinct`. Land birds on 16 small islands had been observed annually during breeding surveys over a period of several decades. Some 62 species went extinct at some point and the objective is to examine the relationship between the years till extinction and three different covariates: the initial average number of nesting pairs observed (`nesting`), the physical size of the birds (an indicator variable `size` with 1=small and 0=large), and migratory status (an indicator variable `status` with 1=resident, 0=migratory).**

**To begin, do the following in `R`.**

```{r}
library(LearnBayes)
data(birdextinct)
n   <- nrow(birdextinct)
extinct.time <- birdextinct$time
avg.no.nests <- birdextinct$nesting
size.ind  <- birdextinct$size   # 0 = large, 1= small
mig.ind   <- birdextinct$status # 0 = mig, 1=resident
size                             <- rep("Small",n)
size[birdextinct$size==0]        <- "Large"
migratory                        <- rep("Resident",n)
migratory[birdextinct$status==0] <- "Migratory"  
```

**3.1. We start by doing some exploratory data analysis and data transformation. Look at the histogram of `extinct.time`. It is strongly right skewed (there are few species with times till extinction that are long relative to most species). Therefore make the response variable the natural log of exinct.time:**

The histogram is shown below. As stated in the problem sheet, the distribution shows a strong right skew --- there is a large peak at short extinction times, and then a long tail of low frequency out to quite large extinction times.

```{r}
hist(extinct.time,xlab="",main="Exinction Times (years)")

log.extinct <- log(extinct.time)
```

**3.2. Make 4 plots of** $y$**=`log.extinct`: histogram of** $y$ **, scatterplot of** $y$ **against `avg.nests`, side-by-side boxplots of** $y$ **for small and large birds, and side-by-side boxplots of resident and migratory birds. Hint: To make side-by-side boxplots use the `split` function; e.g.,`boxplot(split(log.extinct,size),main='vs Size')`**

The suggested summary plots for the log extinction data are shown in the figure below.

```{r}
par(mfrow=c(2,2),oma=c(0,0,3,0))
par(mar=c(2,2,2,2))
#Setting margins
hist(log.extinct,xlab="",main="Log(Extinction)")
scatter.smooth(log.extinct ~ avg.no.nests,xlab="Avg #Nests",ylab="",
      main="vs Abundance")
boxplot(split(log.extinct,size),main="vs Size ")
boxplot(split(log.extinct,migratory),main="vs Residency")
 mtext("Log Time till Extinction",outer=TRUE)

par(mfrow=c(1,1)) 
```

The plots above are Exploratory Data Analysis for the bird extinction data, using the logarithm of the extinction time as the dependent variable. The plots show a histogram of the extinction time (top left), a scatter plot of extinction time versus average number of nests (top right) and box plots of extinction time versus size (bottom left) and versus residence type (bottom right).

**3.3. How would you describe the relationships between the 3 covariates and time till extinction?**

The plot of log-extinction time versus average number of nests shows a clear trend, to increase a the number of nests increases, and the relationship is reasonably linear. There is also a clear trend for the extinction time to decrease for small birds versus large birds, and for it to increase for resident birds versus those that are migratory.

**3.4. Fit a classical multiple linear regression of the `log.extinct` on the three covariates,**

```{r}
extinct.mlr <- lm(log.extinct ~ avg.no.nests + size + migratory)
```

**3.5. Examine the estimated coefficients. How do they compare to your conclusions from the EDA (Exploratory Data Analysis)?**

```{r}
summary(extinct.mlr)
```

As we expected from looking at the figures above, there is a relationship between the time to extinction and all three of the covariates, so all of the fit coefficients are significant. The direction of the trends are also as expected --- extinction time increases with the average number of nests, the size of the bird and if it is resident rather than migratory.

Note we could also centre the average number of nests variable before fitting which gives similar conclusions

```{r}
# Could also centre the average number of nests variable before doing the regression.
nest.ctr <- avg.no.nests-mean(avg.no.nests)
extinct.ctr.mlr <- lm(log.extinct~ nest.ctr+size+migratory)
summary(extinct.ctr.mlr)
par(mfrow=c(2,2))
plot(extinct.mlr)
par(mfrow=c(1,1))
```

**In questions 3.6-3.11, we use JAGS to fit a Bayesian multiple regression analysis.**

**3.6. Implement the Bayesian version of the above multivariate regression model in JAGS. Centre the *avg.no.nests* covariate. Use 3 sets of initial values for the parameters.**

See the `R` code below for the JAGS implementation.

```{r}
# 3.6. Bayesian analysis
#We have computed the centered version of avg.no.nests in the solution to 2.1, and stored it in variable nest.ctr
extinct.model <-   
  "model {
# data that will be read in are n, log.extinct, nest.ctr, size.ind, mig.ind plus prior hyperparameters
# prior
beta0     ~ dnorm(mu.0,tau.0)
beta.nest ~ dnorm(mu.0,tau.0)
beta.size ~ dnorm(mu.0,tau.0)
beta.mig  ~ dnorm(mu.0,tau.0)
tau       ~ dgamma(tau.a,tau.b)

#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta.nest*nest.ctr[i] + beta.size*size.ind[i] + beta.mig*mig.ind[i]
log.extinct[i] ~ dnorm(mu[i],tau)
}

sigma2 <- 1/tau

}"

extinct.data <- list(log.extinct=log.extinct,nest.ctr=nest.ctr,size.ind=size.ind,
                     mig.ind=mig.ind, n=n, mu.0=0, tau.0=1.0e-5, tau.a=0.01, tau.b=0.01)
#Specify three sets of intiial conditions
extinct.inits <- list(list(beta0=0.1,beta.nest=1, beta.size=10,beta.mig=3,tau=0.1),
                      list(beta0=0.2,beta.nest=-5,beta.size=5, beta.mig=1,tau=0.1),
                      list(beta0=0.2,beta.nest=10,beta.size=20,beta.mig=-2,tau=0.2))
num.chains <- 3
results.extinct.A<- jags.model(file=textConnection(extinct.model), 
                                  data=extinct.data , inits=extinct.inits , 
                                  n.chains=num.chains)
#Can also run without specifying initial values for the three chains.
#results.extinct.A<- jags.model(file=textConnection(extinct.model), 
#                                  data=extinct.data, 
#                                  n.chains=num.chains)
update(results.extinct.A, n.iter=1000)
results.extinct.B <- coda.samples(results.extinct.A, 
        variable.names=c("beta0","beta.nest", "beta.size","beta.mig","sigma2"), 
         n.iter=10000)
```

**3.7. Plot the JAGS output to see the trace plots.**

Trace plots for the JAGS fit of this model to the data, after centring the average number of nests, are shown below. These show that the chains are mixing well and the final posterior distributions look smooth and well sampled.

```{r}
#Produce trace plots
par(mfrow=c(3,2))
plot(results.extinct.B[,c("beta.nest", "beta.size","beta.mig")])
par(mfrow=c(2,2))
plot(results.extinct.B[,c("beta0","sigma2")])
par(mfrow=c(1,1))

```

**3.8. Use the Gelman-Rubin diagnostics to check for convergence.**

Below we plot the Gelman-Rubin statistic based on the JAGS fit to the model. The potential scale reduction factors are all equal to $1$ and so we conclude that the results are robust.

```{r}
par(mfrow=c(3,2))
gelman.plot(results.extinct.B)
```

```{r}
gelman.diag(results.extinct.B)
```

**3.9. Plot the autocorrelation functions.**

Below we show plots of the autocorrelation functions from the JAGS fit to this model, showing very little autocorrelation.

```{r}
par(mfrow=c(3,2))
#Setting 3x2 plots displayed next to each other
par(mar=c(1,1,1,1))
#Setting the margins to be small

acf(results.extinct.B[[1]][,"beta0"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.nest"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.size"],lag.max=30)
acf(results.extinct.B[[1]][,"beta.mig"],lag.max=30)
acf(results.extinct.B[[1]][,"sigma2"],lag.max=30)
par(mfrow=c(1,1))
#Changing back to the 1 plot being displayed at a time

```

**3.10. Examine the effective sample sizes, and print out the summary statistics.**

The effective sample sizes and summary statistics are computed below.

```{r}
cat("ESS for beta0:",effectiveSize(results.extinct.B[[1]][,"beta0"]),"\n")     
cat("ESS for beta.nest:",effectiveSize(results.extinct.B[[1]][,"beta.nest"]),"\n") 
cat("ESS for beta.size:",effectiveSize(results.extinct.B[[1]][,"beta.size"]),"\n")
cat("ESS for beta.mig:",effectiveSize(results.extinct.B[[1]][,"beta.mig"]),"\n") 
cat("ESS for sigma2:",effectiveSize(results.extinct.B[[1]][,"sigma2"]),"\n") 
```

```{r}
#Get summary statistics for posterior distributions
summary(results.extinct.B) 
```

**3.11. Calculate studentised residuals, draw a QQ-plot to check normality, plot posterior mean fitted values, and carry out posterior predictive checks for the minimum and maximum log.extinct times. (See the mtcars example in the R code for Lecture 3 on the Learn site for example code to do this.)**

First we compute the studentised residuals. We plot the posterior mean studentised residual for each observation, versus the index numbering that observation. There are no obvious outliers in this plot.

```{r}
#Posterior predictive checks. 
#First compute studentised residuals fitted values
resmat=as.matrix(results.extinct.B)
dim(resmat)
niterf=nrow(resmat)
beta0=resmat[,4]; beta1=resmat[,2] ;beta2=resmat[,3]; beta3=resmat[,1]; sigma2=resmat[,5]
x=cbind(rep(1,n),nest.ctr,size.ind,mig.ind)
H=x%*%solve((t(x)%*%x))%*%t(x)
fittedvalues=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
fittedvalues[,l]=beta0[l]*x[,1]+beta1[l]*x[,2]+beta2[l]*x[,3]+beta3[l]*x[,4]
}

#studentised residuals
studentisedresid=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
 for(i in 1:n){
 studentisedresid[i,l]=(log.extinct[i]-fittedvalues[i,l])/(sqrt(sigma2[l]*(1-diag(H)[i])))
 }
}

#posterior mean of studentised residuals
studentisedresidm=numeric(n)
for(i in 1:n){
studentisedresidm[i]=mean(studentisedresid[i,])
}

#Plot of posterior mean studentised residual versus observation number.
par(mfrow=c(1,1))

plot(seq_along(studentisedresidm),studentisedresidm,xlab="Index",ylab="Bayesian studentised residual",ylim=c(-3,3))



```

In the figure below we show a q-q plot based on the posterior mean studentised residuals. These lie reasonably well on the diagonal line.

```{r}

#QQ-plot
qqnorm(studentisedresidm,xlim=c(-3,3),ylim=c(-3,3),lwd=2)
qqline(studentisedresidm,col=2,lwd=2)

```

In the figure below we plot the studentised residual versus the posterior mean of the fitted value of the model. In this figure we are looking for any trends in the data which might suggest heteroscedastic errors or missing terms in the model for the mean. There are no obvious trends.

```{r}
#Compute posterior mean fitted values
fittedvaluesm=numeric(n)
for(i in 1:n){
fittedvaluesm[i]=mean(fittedvalues[i,])
}

plot(fittedvaluesm,studentisedresidm,xlab="Fitted value (posterior mean)",ylab="Bayesian Studentised residual (posterior mean)")
```

Finally, the figures below show the posterior predictive distribution of the minimum and maximum extinction time. These are estimated by taking a sequence of random draws from the posterior distribution, generating a new data set with observations at the same set of covariate values as the original data set based on the model for the mean specified by that draw from the posterior and then computing the relevant statistic from the mock data set. The histograms show the distribution of the derived statistic over those random realisations. The observed value of the same statistic, i.e. computed from the observed data set, is shown in both panels as a vertical red line. If the model is a good description of the data we expect the statistic value for the observed data set to lie within the histogram of values, i.e., it should not be a significant outlier. The distribution of the maximum extinction time looks OK in that regard, but this is not the case for the minimum extinction time. Looking more closely at the data set we see that there are no extinction times recorded that are less than 1 unit of time, but the linear model to log-extinction will predict values that range between $0$ and $\infty$. This suggests that the linear model is not a good description for small extinction times, so modified versions of the model should be explored.

```{r}
#Now do some predictive checks
#First replicate the data
yrep=matrix(0,nrow=n,ncol=niterf)
for(l in 1:niterf){
  for(i in 1:n){
    yrep[i,l]=rnorm(1,beta0[l]*x[i,1]+beta1[l]*x[i,2]+beta2[l]*x[i,3]+beta3[l]*x[i,4],sigma2[l])
  }
}

#Compute posterior preditive distribution of min amd max log(extinction time).
yrepmin=apply(yrep,2,min)
yrepmax=apply(yrep,2,max)

par(mfrow=c(2,1))
hist(yrepmin,col="gray40",main="Predictive distribution for minimum")
abline(v=min(log.extinct),col="red",lwd=2)
hist(yrepmax,col="gray40",main="Predictive distribution for maximum")
abline(v=max(log.extinct),col="red",lwd=2)  
```

The histograms above show posterior predictive distribution for the minimum (top) and maximum (bottom) log extinction time for the Bayesian fit to the bird extinction time data. The values int he observed data set are shown by vertical red lines. The lines seem to be within the typical range of the replicates, showing a reasonably good fit.

An alternative method would be to create the replicated directly with JAGS. This can be done by adding a new stochastic node $\texttt{log.extinct.replicate[i]~ dnorm(mu[i],tau)}$ that has the same distribution as the observation $\texttt{log.extinct[i]}$ but it is not observed (i.e. not included in the data).

```{r}
def.extinct.model.with.replicates <-   
  "model {
# data that will be read in are n, log.extinct, nest.ctr, size.ind, mig.ind plus prior hyperparameters
# prior
beta0     ~ dnorm(mu.0,tau.0)
beta.nest ~ dnorm(mu.0,tau.0)
beta.size ~ dnorm(mu.0,tau.0)
beta.mig  ~ dnorm(mu.0,tau.0)
tau       ~ dgamma(tau.a,tau.b)

#Likelihood
for(i in 1:n) {
mu[i]  <- beta0+beta.nest*nest.ctr[i] + beta.size*size.ind[i] + beta.mig*mig.ind[i]
log.extinct[i] ~ dnorm(mu[i],tau)
log.extinct.replicate[i]~ dnorm(mu[i],tau)
}

sigma2 <- 1/tau

}"

extinct.data <- list(log.extinct=log.extinct,nest.ctr=nest.ctr,size.ind=size.ind,
                     mig.ind=mig.ind, n=n, mu.0=0, tau.0=1.0e-5, tau.a=0.01, tau.b=0.01)
num.chains <- 3
model.extinct.with.replicates<- jags.model(file=textConnection(def.extinct.model.with.replicates), 
                                  data=extinct.data , 
                                  n.chains=num.chains)
#Can also run without specifying initial values for the three chains.
#results.extinct.A<- jags.model(file=textConnection(extinct.model), 
#                                  data=extinct.data, 
#                                  n.chains=num.chains)

update(model.extinct.with.replicates, n.iter=1000)
results.extinct.with.replicates <- coda.samples(model.extinct.with.replicates, 
    variable.names=c("log.extinct.replicate"),
         n.iter=10000)
#We only save the replicates, and not the other variables "beta0","beta.nest", "beta.size","beta.mig","sigma2"

```

Now the replicates for each data point are contained in the MCMC output, and they can be extracted to do the posterior predictive checks.

```{r}
#Now do some predictive checks using the replicates from JAGS
#First, we convert the output into a matrix
matrix.results.extinct.with.replicates=as.matrix(results.extinct.with.replicates)

#The yrep matrix is easily obtained from this, 
#we need to use transpose t() due to the way we use yrep in the plots
yrep=t(matrix.results.extinct.with.replicates[1:niterf,1:n])

#Compute posterior preditive distribution of min amd max log(extinction time).
yrepmin=apply(yrep,2,min)
yrepmax=apply(yrep,2,max)

par(mfrow=c(2,1))
hist(yrepmin,col="gray40",main="Predictive distribution for minimum")
abline(v=min(log.extinct),col="red",lwd=2)
hist(yrepmax,col="gray40",main="Predictive distribution for maximum")
abline(v=max(log.extinct),col="red",lwd=2)
```

The results are the same as previously (up to some small difference due to randomness).

**4. State space modeling for gray whale abundance**

**In this exercise, we continue work with our state space model from Lecture 3.**

**As a recap, we have some observations of gray whale abundance (i.e. population size) during some of the years in the period 1951-1997. It turns out that it is easier to fit a model on the logarithm of the population rather than the population directly. We are going to use the following models from the true log-population** $x_t$ **(**$t$ **denotes the year),**

$$x_t = b x_{t-1}+u+w_t; \quad w_t\sim N(0,\sigma^2).$$

**The observations are not assumed to be exact, but we allow them to have an error, this is modelled as**

$$y_t =x_{t}+v_t, \quad v_t\sim N(0,\eta^2).$$

**This is a simple State-Space Model (SSM), also called Hidden Markov Model.** $x_t$ **are the hidden states, and** $y_t$ **are the observations.**

The following lines load the dataset and create the observation vector $y$.

```{r}
library("MARSS")
#This loads to MARSS library
#If not available, please use install.packages("MARSS") first before loading it

# We load the graywhales dataset, and print out the values
data(graywhales)
print(graywhales)

n=1997-1951+1
#We will have one y[i] for each year from 1951 until 1997
#So i=1 corresponds to 1951, i=2 corresponds to 1952, etc.

y=rep(NA,n)
for(i in 1:nrow(graywhales)){
    y[graywhales[i,1]-1950]=log(graywhales[i,2])
}
```

**4.1. Implement the model in JAGS using the following prior distributions for the model parameters:**

-   **Gaussian prior** $N(\log(2500),1)$ for the initial state $x_0$,

-   **Uniform prior on the interval (0,1) for parameter** $b$,

-   **Exponential prior with parameter 1 for** $u$,

-   **Inverse Gamma prior with parameters (0.1,0.1) for** $\sigma^2$ and $\eta^2$.

**Hint: you can use the code provided with Lecture 3 for this.**

**Compile and run the JAGS simulations. Compute the effective samples sizes for these 5 parameters. Choose the number of steps in the burn-in period and the number of MCMC iterations in a way to ensure that the effective sample sizes for the 4 parameters** $x_0$,$b$**,** $u$**,** $\sigma^2$ **and** $\eta^2$ **are all above 1000. Once this is ensured, compute summary statistics and plot the posterior densities of these parameters.**

```{r}
n=1997-1951+1
#We will have one y[i] for each year from 1951 until 1997
#So i=1 corresponds to 1951, i=2 corresponds to 1952, etc.

y=rep(NA,n)
for(i in 1:nrow(graywhales)){
    y[graywhales[i,1]-1950]=log(graywhales[i,2])
}
#We let y[i] be NA if the data for that year is not available, and the logarithm of the observed abundance when the data is available

#We create the model string in JAGS
model_string <-   
  "model {
  # prior on the initial true log-abundance at year 1951 (x_0), denoted by x[1] in R
  x[1] ~ dnorm(mean.x0,prec.x0)
  b ~ dunif(bmin,bmax)
  u ~ dexp(lambda.u)
  tau.sigma2 ~ dgamma(a.sigma,b.sigma)
  tau.eta2 ~ dgamma(a.eta,b.eta)
  sigma2 <- 1/tau.sigma2
  eta2 <- 1/tau.eta2

  #Likelihood
  #n denotes the total number of years considered starting from 1951
  for(i in 2:n) {
    x[i] ~ dnorm(b*x[i-1]+u,tau.sigma2)
  }
  for(i in 1:n) {
    y[i] ~ dnorm(x[i],tau.eta2)
    yrep[i] ~ dnorm(x[i],tau.eta2)
    #We also include replicates in the model
  }
}"

#We fix the hyperparameters for the priors in some way
mean.x0=log(2500)
prec.x0=1
bmin=0
bmax=1
lambda.u=1
a.sigma=0.1
b.sigma=0.1
a.eta=0.1
b.eta=0.1

#We create some initial values the Markov chain for the variables x[1],...,x[n] by interpolating the observations y[1],...,y[n] to the positions where the observations are not available (we also add some small random noise).
#This interpolation is convenient by the na_interpolation function from the imputeTS library
library(imputeTS)
x.init=na_interpolation(y)

model.inits <- list(list(b=0.8, u=0.5, tau.sigma2=1, tau.eta2=1, x=x.init+rnorm(n,sd=0.1)),
                   list(b=0.9, u=0.3, tau.sigma2=2, tau.eta2=2, x=x.init+rnorm(n,sd=0.1)),
                   list(b=0.7, u=1, tau.sigma2=0.7, tau.eta2=0.7, x=x.init+rnorm(n,sd=0.1)),
                    list(b=0.6, u=1.5, tau.sigma2=0.5, tau.eta2=0.5, x=x.init+rnorm(n,sd=0.1)),
                    list(b=0.5, u=2, tau.sigma2=0.3, tau.eta2=0.3, x=x.init+rnorm(n,sd=0.1)))

model1.data <- list(n=n,y=y,mean.x0=mean.x0,prec.x0=prec.x0,
                               bmin=bmin,bmax=bmax,lambda.u=lambda.u,
                               a.sigma=a.sigma,b.sigma=b.sigma,
                               a.eta=a.eta,b.eta=b.eta)

# #compiling model
model1=jags.model(textConnection(model_string),data = model1.data,n.chains=5, inits=model.inits)

# # Burnin for 30000 samples
update(model1,30000,progress.bar="none")

# # Running the model, monitoring the variable theta
res.model1=coda.samples(model1,variable.names=c("b","u","sigma2","eta2","x[1]"), n.iter=170000,progress.bar="none")
#Display some summary statistics
summary(res.model1)
```

```{r}
effectiveSize(res.model1)
```

```{r}
plot(res.model1)
```

**4.2. Now we are going to perform prior sensitivity checks. Try a different prior distribution for the 5 variables. For** $x_0$**, please do not change the mean significantly from log(2500), but you can try a different variance. When formulating priors, you are encouraged to use your understanding of the model parameters and biological facts (see e.g. <https://en.wikipedia.org/wiki/Gray_whale>). Run the simulations again, and compare the summary statistics and posterior density plots of these 5 parameters.**

To choose the priors on the model parameters in a more informed fashion, we have looked at

the Wikipedia page for gray whales, <https://en.wikipedia.org/wiki/Gray_whale.>

It is stated that they typically live for 55-70 years, and females give birth to a single calf

weighting around 1 tonne typically every 2 years during their adult life.

If approximately half of the whales are females, and approximately one third of them gives

birth each year, then the number of newborns per year is approximately 1/6'th of the

population. So choosing the parameter $u\approx 6$ seems reasonable, and for this reason, we

choose an exponential prior with parameter lambda.u=6 (expected value according to the

prior is 1/ lambda.u=1/6). The parameter $b$ is related to the decrease in the population due to

deaths. As the typical life time of gray whales that make it to adulthood is 55-70 years, we

feel that this cannot be very low, so instead of a uniform prior on [0,1], we set a uniform prior

on [0.5, 1] in this case. For the priors on the variances $\sigma^2$ and $\eta^2$, we choose

Inverse Gamma (1, 1/20). This is much more concentrated, with most of the mass contained

in the interval [0,0.1] (corresponding to lower noise). We have also increased the precision of the Gaussian prior on $x_0$ to 2, while kept the mean at $\log(2500)$.

```{r}
mean.x0=log(2500)
prec.x0=2
bmin=0.5
bmax=1
lambda.u=6
a.sigma=1
b.sigma=1/20
a.eta=1
b.eta=1/20

model2.data <- list(n=n,y=y,mean.x0=mean.x0,prec.x0=prec.x0,
                               bmin=bmin,bmax=bmax,lambda.u=lambda.u,
                               a.sigma=a.sigma,b.sigma=b.sigma,
                               a.eta=a.eta,b.eta=b.eta)

# #compiling model
model2=jags.model(textConnection(model_string),data = model2.data,n.chains=5, inits=model.inits)

# # Burnin for 30000 samples
update(model2,30000,progress.bar="none")

# # Running the model, monitoring the variable theta
res.model2=coda.samples(model2,variable.names=c("b","u","sigma2","eta2","x[1]"), n.iter=170000,progress.bar="none")
#Display some summary statistics
summary(res.model2)

```

```{r}
plot(res.model2)
```

As we can see, the posterior distributions of these 5 parameters do change rather significantly compared to the first prior, especially for the two variance parameters $\eta^2$ and $\sigma^2$. Hence the posterior is rather sensitive with respect to the prior choice, which is not surprising as we only have 24 observations in this example.

**4.3.** **Update the model to compute the posterior distribution of the log population sizes (**$x_t$**) every year between 1951-2050. Plot the evolution of the posterior mean of the log population sizes from 1951-2050 on a single plot, along with two other curves that correspond to the [2.5%, 97.5%] credible interval of the log population size (**$x_t$**) according to the posterior distribution at each year. Finally, estimate the posterior probability that the population of gray whales becomes smaller than 100 at any year from 1951 until the end of 2050 (i.e.,** $p(\min_{t\in \{0,1\ldots,99\}} x_t<=\log(100)|y)$**).**

```{r}
n3=100
y3=rep(NA,n3)
y3[1:n]=y

require(imputeTS)
x.init3=na_interpolation(y3)
#We create the initial values of the Markov chain
#by interpolating the available observed log population sizes
#The na_interpolation function replaces the NA values by interpolated values

mean.x0=log(2500)
prec.x0=2
bmin=0.5
bmax=1
lambda.u=6
a.sigma=1
b.sigma=1/20
a.eta=1
b.eta=1/20

model3.data <- list(n=n3,y=y3,mean.x0=mean.x0,prec.x0=prec.x0,
                               bmin=bmin,bmax=bmax,lambda.u=lambda.u,
                               a.sigma=a.sigma,b.sigma=b.sigma,
                               a.eta=a.eta,b.eta=b.eta)

model3.inits <- list(list(b=0.8, u=0.5, tau.sigma2=1, tau.eta2=1, x=x.init3+rnorm(n3,sd=0.1)),
                   list(b=0.9, u=0.3, tau.sigma2=2, tau.eta2=2, x=x.init3+rnorm(n3,sd=0.1)),
                   list(b=0.7, u=1, tau.sigma2=0.7, tau.eta2=0.7, x=x.init3+rnorm(n3,sd=0.1)),
                    list(b=0.6, u=1.5, tau.sigma2=0.5, tau.eta2=0.5, x=x.init3+rnorm(n3,sd=0.1)),
                    list(b=0.5, u=2, tau.sigma2=0.3, tau.eta2=0.3, x=x.init3+rnorm(n3,sd=0.1)))

# compiling model
model3=jags.model(textConnection(model_string),data = model3.data,n.chains=5, inits=model3.inits)

# Burnin for 30000 samples
update(model3,30000,progress.bar="none")

# Running the model
res.model3=coda.samples(model3,variable.names=
           c("b","u","sigma2","eta2","x"), n.iter=170000,progress.bar="none")
```

```{r}
xres=data.frame(rbind(res.model3[[1]][,5:104],res.model3[[2]][,5:104],res.model3[[3]][,5:104],res.model3[[4]][,5:104],res.model3[[5]][,5:104]))
#We combine the results from all chains into a single dataframe

x.mean=apply(xres,MARGIN=2, FUN=mean)
x.q025=apply(xres, MARGIN=2, FUN=function(x) quantile(x,prob=0.025))
x.q975=apply(xres, MARGIN=2, FUN=function(x) quantile(x,prob=0.975))
#We compute the posterior means and the 95% credible interval by the apply function
             
#Alternatively, we could extract these from the summary
#sum.res.model3=summary(res.model3)
#x.mean=sum.res.model3$statistics[5:104,1]
#x.q025=sum.res.model3$quantiles[5:104,1]
#x.q975=sum.res.model3$quantiles[5:104,5]

plot(1951:2050, x.mean,type="l",ylim=c(7.5,14),main="Posterior mean and 95% credible intervals for log population size", xlab="Year",ylab="Log population size")
lines(1951:2050, x.q025,lty=3,col="dark red")
lines(1951:2050, x.q975,lty=3,col="dark red")
```

```{r}
xres.min=apply(xres,MARGIN=1, FUN=min)
sum(xres.min<log(100))
```

**4.4. We are going to perform posterior predictive checks to evaluate the fit of this model on the data. First, create replicate observations from the posterior predictive using JAGS. The number of replicate observations should be at least 1000. Compute the minimum, maximum, median, kurtosis and skewness for the replicated observations. Plot the histograms for these quantities together with a line that shows the value of the function considered on the actual dataset (see the R code for Lecture 3 for an example). Perform these checks both for the original prior from Question 5.1, and the prior you have proposed in Question 5.2. Discuss the results.**

We start by the original prior.

```{r}
ind.yrep.not.NA=which(!is.na(y))
y.not.NA=y[!is.na(y)]

res.model1.yrep=coda.samples(model1,variable.names=c("yrep"), n.iter=100000,progress.bar="none")

yrep.samples=data.frame(rbind(res.model1.yrep[[1]][,ind.yrep.not.NA],res.model1.yrep[[2]][,ind.yrep.not.NA],
                              res.model1.yrep[[3]][,ind.yrep.not.NA],res.model1.yrep[[4]][,ind.yrep.not.NA],
                              res.model1.yrep[[5]][,ind.yrep.not.NA]))
yrep.samples.min=apply(yrep.samples,MARGIN=1, FUN=min)
yrep.samples.max=apply(yrep.samples,MARGIN=1, FUN=max)
yrep.samples.median=apply(yrep.samples,MARGIN=1, FUN=median)
require(fBasics)

yrep.samples.kurtosis=apply(yrep.samples,MARGIN=1, FUN=kurtosis)
yrep.samples.skewness=apply(yrep.samples,MARGIN=1, FUN=skewness)

par(mfrow=c(3,2))
hist(yrep.samples.min,col="gray40",main="Predictive distribution for min")
abline(v=min(y.not.NA),col="red",lwd=2)
hist(yrep.samples.max,col="gray40",main="Predictive distribution for max")
abline(v=max(y.not.NA),col="red",lwd=2)
hist(yrep.samples.median,col="gray40",main="Predictive distribution for median")
abline(v=median(y.not.NA),col="red",lwd=2)
hist(yrep.samples.kurtosis,col="gray40",main="Predictive distribution for kurtosis")
abline(v=kurtosis(y.not.NA),col="red",lwd=2)
hist(yrep.samples.skewness,col="gray40",main="Predictive distribution for skewness")
abline(v=skewness(y.not.NA),col="red",lwd=2)
par(mfrow=c(1,1))
```

Now we perform the same for the prior we have proposed in question 5.2.

```{r}
ind.yrep.not.NA=which(!is.na(y))
y.not.NA=y[!is.na(y)]

res.model2.yrep=coda.samples(model2,variable.names=c("yrep"), n.iter=100000,progress.bar="none")

yrep.samples=data.frame(rbind(res.model2.yrep[[1]][,ind.yrep.not.NA],res.model2.yrep[[2]][,ind.yrep.not.NA],
                              res.model2.yrep[[3]][,ind.yrep.not.NA],res.model2.yrep[[4]][,ind.yrep.not.NA],
                              res.model2.yrep[[5]][,ind.yrep.not.NA]))
yrep.samples.min=apply(yrep.samples,MARGIN=1, FUN=min)
yrep.samples.max=apply(yrep.samples,MARGIN=1, FUN=max)
yrep.samples.median=apply(yrep.samples,MARGIN=1, FUN=median)
require(fBasics)
yrep.samples.kurtosis=apply(yrep.samples,MARGIN=1, FUN=kurtosis)
yrep.samples.skewness=apply(yrep.samples,MARGIN=1, FUN=skewness)

par(mfrow=c(3,2))
hist(yrep.samples.min,col="gray40",main="Predictive distribution for min")
abline(v=min(y.not.NA),col="red",lwd=2)
hist(yrep.samples.max,col="gray40",main="Predictive distribution for max")
abline(v=max(y.not.NA),col="red",lwd=2)
hist(yrep.samples.median,col="gray40",main="Predictive distribution for median")
abline(v=median(y.not.NA),col="red",lwd=2)
hist(yrep.samples.kurtosis,col="gray40",main="Predictive distribution for kurtosis")
abline(v=kurtosis(y.not.NA),col="red",lwd=2)
hist(yrep.samples.skewness,col="gray40",main="Predictive distribution for skewness")
abline(v=skewness(y.not.NA),col="red",lwd=2)
```

The posterior predictive checks do not detect any issues for neither of the two priors.

Another way to evaluate the fit of the model is the DIC criteria. We evaluate the DIC for both the original prior, and the prior proposed in question 5.2.

```{r}
dic.samples(model1, n.iter=100000)
```

```{r}
dic.samples(model2, n.iter=100000)
```

We can see that the alternative prior that we have chosen in part 5.2. based on external information results in a significantly smaller DIC value, indicating better fit on the data.
