}
}
points
}
liverpool.win.prob <- rep(0, 7)
for (i in 1: 7) {  # (5, 10, 15, 20, 25, 30, 35)
r <- 5 * i  # rounds
data.r <- data
data.r[(10 * r + 1): nG, ]$y <- NA
data.r[(nG + 10 * r + 1): (2 * nG), ]$y <- NA
football.I.r <- inla(y ~ attack + defense + home, data=data.r, family='Poisson', control.predictor=list(compute=TRUE), control.compute=list(config=TRUE))
nsamp <- 10000
football.samp.r <- inla.posterior.sample(n=nsamp, result=football.I.r)
fittedvalues.samples.r <- inla.posterior.sample.eval(function(...) {Predictor}, football.samp.r)
scores <- matrix(0, nrow=2 * nG, ncol=nsamp)
# already settled ones
scores[1: (10 * r), ] <- rep(data.r[1: (10 * r), "y"], nsamp)
scores[(nG + 1): (nG + 10 * r), ] <- rep(data.r[(nG + 1): (nG + 10 * r), "y"], nsamp)
# sampled ones
# different poisson rates for attack/defense teams
rates.A <- exp(fittedvalues.samples.r[(10 * r + 1): nG, ])  # due to log links
rates.A.vector <- as.vector(rates.A)
print(nrow(as.matrix(rpois(length(rates.A.vector), rates.A.vector), nrow=nG - 10 * r, ncol=nsamp)))
print(dim(scores[(10 * r + 1): nG, ]))
scores[(10 * r + 1): nG, ] <- as.matrix(rpois(length(rates.A.vector), rates.A.vector), nrow=nG - 10 * r, ncol=nsamp)
rates.D <- exp(fittedvalues.samples.r[(nG + 10 * r + 1): (2 * nG), ])
rates.D.vector <- as.vector(rates.D)
scores[(nG + 10 * r + 1): (2 * nG), ] <- as.matrix(rpois(length(rates.D.vector), rates.D.vector), nrow=nG - 10 * r, ncol=nsamp)
liverpool.champion <- rep(0, nsamp)
for (j in 1: nsamp) {
points <- compute_scores(scores[, j], full.data$HomeTeam, full.data$AwayTeam)
if (points[10] == max(points)) liverpool.champion[j] <- 1
}
liverpool.win.prob[i] <- mean(liverpool.champion)
}
plot(x=seq(from=5, to=35, by=5), y=liverpool.win.prob, type="l", main='Posterior probability that Liverpool is champion vs round number',
xlab="Round number", ylab="Posterior probability that Liverpool is the champion")
compute_scores <- function(scores, home.t, away.t) {
nG <- length(scores) / 2
points <- rep(0, 20)
home.teams <- as.integer(as.factor(home.t))
away.teams <- as.integer(as.factor(away.t))
for (i in 1: nG) {
a <- home.teams[i]
b <- away.teams[i]
if (scores[i] > scores[nG + i]) points[a] <- points[a] + 3
else if (scores[i] < scores[nG + i]) points[b] <- points[b] + 3
else {
points[a] <- points[a] + 1
points[b] <- points[b] + 1
}
}
points
}
liverpool.win.prob <- rep(0, 7)
for (i in 1: 7) {  # (5, 10, 15, 20, 25, 30, 35)
r <- 5 * i  # rounds
data.r <- data
data.r[(10 * r + 1): nG, ]$y <- NA
data.r[(nG + 10 * r + 1): (2 * nG), ]$y <- NA
football.I.r <- inla(y ~ attack + defense + home, data=data.r, family='Poisson', control.predictor=list(compute=TRUE), control.compute=list(config=TRUE))
nsamp <- 10000
football.samp.r <- inla.posterior.sample(n=nsamp, result=football.I.r)
fittedvalues.samples.r <- inla.posterior.sample.eval(function(...) {Predictor}, football.samp.r)
scores <- matrix(0, nrow=2 * nG, ncol=nsamp)
# already settled ones
scores[1: (10 * r), ] <- rep(data.r[1: (10 * r), "y"], nsamp)
scores[(nG + 1): (nG + 10 * r), ] <- rep(data.r[(nG + 1): (nG + 10 * r), "y"], nsamp)
# sampled ones
# different poisson rates for attack/defense teams
rates.A <- exp(fittedvalues.samples.r[(10 * r + 1): nG, ])  # due to log links
rates.A.vector <- as.vector(rates.A)
print(nrow(as.matrix(rpois(length(rates.A.vector), rates.A.vector), nrow=nG - 10 * r, ncol=nsamp)))
print(dim(scores[(10 * r + 1): nG, ]))
scores[(10 * r + 1): nG, ] <- as.matrix(rpois(length(rates.A.vector), rates.A.vector), nrow=nG - 10 * r, ncol=nsamp)
rates.D <- exp(fittedvalues.samples.r[(nG + 10 * r + 1): (2 * nG), ])
rates.D.vector <- as.vector(rates.D)
scores[(nG + 10 * r + 1): (2 * nG), ] <- as.matrix(rpois(length(rates.D.vector), rates.D.vector), nrow=nG - 10 * r, ncol=nsamp)
liverpool.champion <- rep(0, nsamp)
for (j in 1: nsamp) {
points <- compute_scores(scores[, j], full.data$HomeTeam, full.data$AwayTeam)
if (points[10] == max(points)) liverpool.champion[j] <- 1
}
liverpool.win.prob[i] <- mean(liverpool.champion)
}
plot(x=seq(from=5, to=35, by=5), y=liverpool.win.prob, type="l", main='Posterior probability that Liverpool is champion vs round number',
xlab="Round number", ylab="Posterior probability that Liverpool is the champion")
library(INLA)
#If it loaded correctly, you should see this in the output:
#Loading required package: Matrix
#Loading required package: sp
#This is INLA_23.09.09 built 2023-10-16 17:29:11 UTC.
# - See www.r-inla.org/contact-us for how to get help.
#Installation instructions for various operating systems are explained on  https://www.r-inla.org/download-install
#The following code does the full installation. You can try it if INLA has not been installed.
#First installing some of the dependencies (especially on Windows)
#Note that the installation may stop several times, and a pop-up window could appear, where you need to select installation from source or binary (usually binary is ok).
#install.packages("BiocManager")
#BiocManager::install("Rgraphviz")
#if (!requireNamespace("BiocManager", quietly = TRUE))
#    install.packages("BiocManager")
#BiocManager::install("graph")
#
#Installing INLA
#install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
#Loading INLA
#library(INLA)
library(Stat2Data)
#If not available, run this first:
#install.packages("Stat2Data")
data("LongJumpOlympics")   #Makes the dataset available in this R session
Jump <- LongJumpOlympics$Gold
Year <- LongJumpOlympics$Year
#You can get more info about the dataset by the command
#help(topic="LongJumpOlympics",package="Stat2Data")
#1.1. Fit simple linear regression model.
m1 <- lm(Jump ~ Year)
summary(m1)
#Plot data with fit overlaid.
par(mfrow=c(1,1))
plot(Jump ~ Year,xlab="Year",ylab="Jump (m)",
type="n",main="Olympic Gold LongJump Distances (m)",cex=0.6)
text(x=Year,y=Jump,labels=Year)
abline(m1,col="blue")
meanY <- mean(Year)
Year.ctr=Year-meanY
prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
data=data.frame(Jump,Year.ctr)
m1I <- inla(Jump ~Year.ctr,data=data,control.family=list(hyper=prec.prior),control.fixed=prior.beta)
summary(m1I)
m1I$marginals.fixed$`(Intercept)`
plot(m1I$marginals.fixed$`(Intercept)`, type ="l",xlab="x",ylab="Density",
main='Posterior density of beta0')
plot(m1I$marginals.fixed$Year.ctr, type ="l",xlab="x",ylab="Density",
main='Posterior density of beta1')
#By writing
#names(m1I$marginals.hyperpar)
#we can find out that the precision parameter can be accessed as
#m1I$marginals.hyperpar[[1]]
#m1I$marginals.hyperpar$`Precision for the Gaussian observations` would work too
marg.sigma <- inla.tmarginal(function(tau) tau^(-1/2),
m1I$marginals.hyperpar[[1]])
#Summary statistics of sigma
cat("Summary statistics of sigma\n")
inla.zmarginal(marg.sigma)
#Plot of marginal of sigma
plot(marg.sigma, type ="l",xlab="x",ylab="Density",
main='Posterior density of sigma')
meanY <- mean(Year)
Year.ctr=Year-meanY
Year.ctr.2021=c(Year.ctr,2021-meanY)
Jump.2021=c(Jump,NA)
#We are using the same priors as in (ii)
data.2021=data.frame(Jump=Jump.2021,Year.ctr=Year.ctr.2021)
data.2021[21:27, ]
m2I <- inla(Jump ~ Year.ctr,data=data.2021,control.family=list(hyper=prec.prior),
control.fixed=prior.beta,control.predictor = list(compute = TRUE), control.compute = list(config = TRUE))
summary(m2I)
n=length(Year.ctr)
nbsamp=10000;
m2I.samp=inla.posterior.sample(n=nbsamp, result=m2I,selection= list(Predictor=27))
#Obtain the samples from the linear predictors, which is equivalent to the mean of the observations as the link function is the identity here
predictor.samples=unlist(lapply(m2I.samp, function(x)(x$latent[1])))
#We obtain the samples from the parameter sigma using the samples from the precision
sigma.samples=1/sqrt(unlist(lapply(m2I.samp, function(x)(x$hyperpar[1]))))
#We obtain the posterior predictive samples by adding the Gaussian noise from the likelihood to the mean (mu_i=eta_i)
post.pred.samples=predictor.samples+rnorm(nbsamp, mean=0,sd=sigma.samples)
plot(density(post.pred.samples),type ="l",xlab="x",ylab="Density",
main='Posterior predictive density of winning jump length in 2021')
meanY <- mean(Year)
Year.ctr=Year-meanY
Year.ctr.2028=c(Year.ctr,2021-meanY,2024-meanY,2028-meanY)
Jump.2028=c(Jump,NA,NA,NA)
#We are using the same priors as in (ii)
m3I <- inla(Jump.2028 ~ Year.ctr.2028,data=data.frame(Jump=Jump.2028,Year=Year.ctr.2028),
control.family=list(hyper=prec.prior),control.fixed=prior.beta,control.predictor = list(compute = TRUE),
control.compute = list(config = TRUE))
summary(m3I)
#We obtain samples from the linear predictors, and only use the ones that correspond to the 3 additional years 2021, 2024 and 2028
nbsamp=10000
m3I.samp <- inla.posterior.sample(nbsamp, m3I,selection = list(Predictor=27:29))
#Obtain the samples from the linear predictors, which is equivalent to the mean of the observations as the link function is the identity here
predictor.samples=matrix(unlist(lapply(m3I.samp, function(x)(x$latent[1:3]))),nrow=3,ncol=nbsamp)
#We obtain the samples from the parameter sigma using the samples from the precision
sigma.samples=1/sqrt(unlist(lapply(m3I.samp, function(x)(x$hyperpar[1]))))
#We obtain the posterior predictive samples by adding the Gaussian noise from the likelihood to the mean (mu_i=eta_i)
post.pred.samples=matrix(0,nrow=3,ncol=nbsamp)
for(it in 1:3){
post.pred.samples[it,]=predictor.samples[it,]+rnorm(nbsamp, mean=0,sd=sigma.samples)
}
#We compute tha max of each column by sapply, this will be the max among the 3 years for each sample
max.post.pred.samples=sapply(data.frame(post.pred.samples),max)
#We print out the result
p.1968.exceeded=mean(max.post.pred.samples>Jump.2028[16])
cat("Posterior probability of exceeding the record of 1968:",p.1968.exceeded)
meanY <- mean(Year)
Year.ctr=Year-meanY
#Setting the priors for Student's t distribution
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(initial=log(3-2), fixed=TRUE) )
prior.beta <- list(mean.intercept = 8, prec.intercept = 1,
mean = 0, prec = 10)
m4I <- inla(Jump ~ Year.ctr,family="T",data=data.frame(Jump,Year.ctr),control.family=list(hyper=prior.t),control.fixed=prior.beta, control.inla=list(cmin=0))
summary(m4I)
plot(m4I$marginals.fixed$`(Intercept)`, type ="l",xlab="x",ylab="Density",
main='Posterior density of beta0 for robust regression with dof=3')
plot(m4I$marginals.fixed$Year.ctr, type ="l",xlab="x",ylab="Density",
main='Posterior density of beta1 for robust regression with dof=3')
names(m4I$marginals.hyperpar)
#we can find out the name of the precision parameter from the summary above, or by running
marg.robust.sigma <- inla.tmarginal(function(tau) tau^(-1/2),
m4I$marginals.hyperpar$`precision for the student-t observations`)
#Summary statistics of sigma
cat("Summary statistics of sigma for robust regression with dof=3\n")
inla.zmarginal(marg.robust.sigma)
#Plot of marginal of sigma
plot(marg.robust.sigma, type ="l",xlab="x",ylab="Density",
main='Posterior density of sigma for robust regression with dof=3')
meanY <- mean(Year)
Year.ctr=Year-meanY
#Setting the priors for Student's t distribution
#The reason that we are writing "loggamma" instead of "gamma" is that
#INLA has a logarithmic internal parametrisation for most of the hyperparameters,
#including precision and degrees of freedom
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(prior = "pc.dof",param = c(15,0.5)) )
prior.beta <- list(mean.intercept = 0, prec.intercept = 1,
mean = 0, prec = 1)
m5I <- inla(Jump ~ Year.ctr,family="T",data=data.frame(Jump,Year.ctr),control.family=list(hyper=prior.t),control.fixed=prior.beta, control.inla=list(cmin=0))
summary(m5I)
#You can find out that this marginal is called `degrees of freedom for student-t`
#by the summary above, or from
names(m5I$marginals.hyper)
plot(m5I$marginals.hyper$`degrees of freedom for student-t`, type ="l",xlab="x",ylab="Density",xlim=c(2,15),ylim=c(0,0.1),
main='Posterior density of dof parameter nu for robust regression')
meanY <- mean(Year)
Year.ctr=Year-meanY
data=data.frame(Jump,Year.ctr)
prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
m1I <- inla(Jump ~Year.ctr,data=data,
control.family=list(hyper=prec.prior),control.fixed=prior.beta,control.compute=list(cpo=TRUE, dic=TRUE))
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(initial=log(3-2), fixed=TRUE) )
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
m4I <- inla(Jump ~ Year.ctr,family="T",data=data,control.family=list(hyper=prior.t),control.fixed=prior.beta,
control.compute=list(cpo=TRUE, dic=TRUE),control.inla=list(cmin=0))
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(prior = "pc.dof",param = c(15,0.5)))
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
m5I <- inla(Jump ~ Year.ctr,family="T",data=data.frame(Jump,Year.ctr),control.family=list(hyper=prior.t),control.fixed=prior.beta,
control.compute=list(cpo=TRUE, dic=TRUE),control.inla=list(cmin=0))
cat("Log marginal likelihood of model 1:",m1I$mlik[1],"\n")
cat("Log marginal likelihood of model 2:",m4I$mlik[1],"\n")
cat("Log marginal likelihood of model 3:",m5I$mlik[1],"\n")
cat("DIC of model 1:",m1I$dic$dic,"\n")
cat("DIC of model 2:",m4I$dic$dic,"\n")
cat("DIC of model 3:",m5I$dic$dic,"\n")
cat("NSLCPO of model 1:",-sum(log(m1I$cpo$cpo)),"\n")
cat("NSLCPO of model 2:",-sum(log(m4I$cpo$cpo)),"\n")
cat("NSLCPO of model 3:",-sum(log(m5I$cpo$cpo)),"\n")
#We load the Taipei house price dataset
house=read.csv("Real_estate.csv")
head(house)
house$transaction=scale(house$X1.transaction.date)
house$age=scale(house$X2.house.age)
house$distance=scale(house$X3.distance.to.the.nearest.MRT.station)
house$stores=scale(house$X4.number.of.convenience.stores)
house$latitude=scale(house$X5.latitude)
house$longitude=scale(house$X6.longitude)
house$y=log(house$Y.house.price.of.unit.area)
m=lm(y~transaction+age+distance+stores+latitude+longitude,data=house)
summary(m)
prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.beta <- list(mean = 0, prec = 0.01,
mean = 0, prec = 0.01)
m.I=inla(y~transaction+age+distance+stores+latitude+longitude,data=house,
family="Gaussian",
control.predictor = list(compute = TRUE),
control.compute = list(cpo=TRUE,config = TRUE),
control.family=list(hyper=prec.prior),control.fixed=prior.beta)
summary(m.I)
marg.sigma2 <- inla.tmarginal(function(tau) tau^(-1),
m.I$marginals.hyperpar[[1]])
#Summary statistics of sigma2
cat("Summary statistics of sigma2\n")
inla.zmarginal(marg.sigma2)
#Plot of marginal of sigma
plot(marg.sigma2, type ="l",xlab="x",ylab="Density",
main='Posterior density of sigma2 for model 1')
m.I.nlscpo=-sum(log(m.I$cpo$cpo))
cat("NLSCPO of model 1:",m.I.nlscpo,"\n")
cat("Standard deviation of mean residuals for model 1:",sd(house$y-m.I$summary.fitted.values$mean),"\n")
nbsamp=10000
samp <- inla.posterior.sample(nbsamp, m.I)
sigma=1/sqrt(inla.posterior.sample.eval(function(...) {theta},
samp))
#In this model the link function is the identity, so fitted values are the same as the linear predictors
#(E(y_i|x,theta)=mu_i=eta_i)
fittedvalues=inla.posterior.sample.eval(function(...) {Predictor},
samp)
n=nrow(house)
x=cbind(rep(1,n),house$transaction,house$age,house$distance,house$stores,house$latitude,house$longitude)
H=x%*%solve((t(x)%*%x))%*%t(x)
#studentised residuals
#n is the number of rows in the dataset, i.e. the number of observations
#studentised residuals
studentisedred=matrix(0,nrow=n,ncol=nbsamp)
#create a matrix of size n * nbsamp, repeating y in each column
y=house$y
ymx=as.matrix(y)%*%matrix(1,nrow=1,ncol=nbsamp);
studentisedred=ymx-fittedvalues;
for(l in 1:nbsamp){
studentisedred[,l]=studentisedred[,l]/sigma[l];
}
for(i in 1:n){
studentisedred[i,]=studentisedred[i,]/sqrt(1-H[i,i]);
}
#posterior mean of studentised residuals
studentisedredm=numeric(n)
for(i in 1:n){
studentisedredm[i]=mean(studentisedred[i,])
}
#Plot of posterior mean studentised residual versus observation number.
par(mfrow=c(1,1))
plot(seq_along(studentisedredm),studentisedredm,xlab="Index",ylab="Bayesian studentised residual",ylim=c(-3,3))
#Compute posterior mean fitted values
fittedvaluesm=numeric(n)
for(i in 1:n){
fittedvaluesm[i]=mean(fittedvalues[i,])
}
plot(fittedvaluesm,studentisedredm,xlab="Fitted value (posterior mean)",ylab="Bayesian Studentised residual (posterior mean)")
#QQ-plot
qqnorm(studentisedredm,xlim=c(-3,3),ylim=c(-3,3),lwd=2)
qqline(studentisedredm,col=2,lwd=2)
#We use Gamma(0.1,0.1) prior for the precision, and fix dof at 3
#Note that this has to be done according to the internal parametrisation as we have seen in Lecture 3 and Workshop 3
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(initial=log(3-2), fixed=TRUE) )
prior.beta <- list(mean.intercept = 0, prec.intercept = 1e-2,
mean = 0, prec = 1e-2)
m.I2=inla(y~transaction+age+distance+stores+longitude+latitude,
data=house,family="T",control.compute=list(cpo=T),
control.family=list(hyper=prior.t),control.fixed=prior.beta)
summary(m.I2)
m.I2.nlscpo=-sum(log(m.I2$cpo$cpo))
cat("NLSCPO of model 2:",m.I2.nlscpo,"\n")
cat("Standard deviation of mean residuals for model 2:",sd(house$y-m.I2$summary.fitted.values$mean),"\n")
marg.sigma2 <- inla.tmarginal(function(tau) tau^(-1),
m.I2$marginals.hyperpar[[1]])
plot(marg.sigma2, type ="l",xlab="x",ylab="Density",
main='Posterior density of sigma2 for model 2')
#Degrees of freedom parameter nu is fixed at 3, so no density plot is needed
#If you had put a prior on it instead, you can plot its density by
#plot(m.I2$marginals.hyperpar[[2]], type ="l",xlab="x",ylab="Density",
#main='Posterior density of dof parameter nu for model 2',xlim=c(2,10))
marg.sigma2
prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.beta <- list(mean.intercept = 0, prec.intercept = 1,
mean = 0, prec = 1)
m.I3=inla(y~transaction+age+distance+stores+longitude+latitude+I(longitude*longitude)+I(latitude*longitude)+I(latitude*latitude),
data=house,family="Gaussian",control.compute=list(cpo=T,dic=T),
control.family=list(hyper=prec.prior),control.fixed=prior.beta)
summary(m.I3)
m.I3.nlscpo=-sum(log(m.I3$cpo$cpo))
cat("NLSCPO of model 3:",m.I3.nlscpo,"\n")
cat("DIC of model 3:",m.I3$dic$dic,"\n")
cat("Standard deviation of mean residuals for model 3:",sd(house$y-m.I3$summary.fitted.values$mean),"\n")
prior.t <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)), dof = list(initial=log(3-2), fixed=TRUE) )
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
m.I4=inla(y~transaction+age+distance+stores+longitude+latitude+I(longitude*longitude)+I(latitude*longitude)+I(latitude*latitude),
data=house,family="T",control.compute=list(cpo=T,dic=T),
control.family=list(hyper=prior.t),control.fixed=prior.beta,control.inla=list(cmin=0))
summary(m.I4)
m.I4.nlscpo=-sum(log(m.I4$cpo$cpo))
cat("NLSCPO of model 4:",m.I4.nlscpo,"\n")
cat("DIC of model 4:",m.I4$dic$dic,"\n")
cat("Standard deviation of mean residuals for model 4:",sd(house$y-m.I4$summary.fitted.values$mean),"\n")
n=nrow(house)
house.predict=rbind(house,house,house,house)
mean.transaction=mean(house$X1.transaction.date)
sd.transaction=sd(house$X1.transaction.date)
mean.age=mean(house$X2.house.age)
sd.age=sd(house$X2.house.age)
house.predict$y[(n+1):(4*n)]=NA
house.predict$transaction[(n+1):(2*n)]=(2014.0-mean.transaction)/sd.transaction
house.predict$transaction[(2*n+1):(3*n)]=(2015.0-mean.transaction)/sd.transaction
house.predict$transaction[(3*n+1):(4*n)]=(2016.0-mean.transaction)/sd.transaction
house.predict$age[(n+1):(2*n)]=house$age+(2014.0-house$X1.transaction.date)/sd.age
house.predict$age[(2*n+1):(3*n)]=house$age+(2015.0-house$X1.transaction.date)/sd.age
house.predict$age[(3*n+1):(4*n)]=house$age+(2016.0-house$X1.transaction.date)/sd.age
prec.prior <- list(prec=list(prior = "loggamma", param = c(0.1, 0.1)))
prior.beta <- list(mean.intercept = 0, prec.intercept = 0.01,
mean = 0, prec = 0.01)
m.I5=inla(y~transaction+age+distance+stores+longitude+latitude,
data=house.predict,family="Gaussian",control.predictor=list(compute=TRUE),control.compute=list(config=TRUE),
control.family=list(hyper=prec.prior),control.fixed=prior.beta)
nbsamp=30000
samp <- inla.posterior.sample(nbsamp, m.I5)
sigma.samples=1/sqrt(inla.posterior.sample.eval(function(...) {theta},
samp))
#In this model the link function is the identity, so fitted values are the same as the linear predictors
#(E(y_i|x,theta)=mu_i=eta_i)
predictor.samples=inla.posterior.sample.eval(function(...) {Predictor},
samp)
post.pred.samples=matrix(0,nrow=(3*n),ncol=nbsamp)
for(it in 1:(3*n)){
post.pred.samples[it,]=predictor.samples[n+it,]+rnorm(nbsamp, mean=0,sd=sigma.samples)
}
post.pred.house.price.samples=exp(post.pred.samples)
average.house.price.samples.2014=apply(post.pred.house.price.samples[1:n,], MARGIN=2, FUN=mean)
average.house.price.samples.2015=apply(post.pred.house.price.samples[(n+1):(2*n),], MARGIN=2, FUN=mean)
average.house.price.samples.2016=apply(post.pred.house.price.samples[(2*n+1):(3*n),], MARGIN=2, FUN=mean)
plot(density(average.house.price.samples.2014))
plot(density(average.house.price.samples.2015))
plot(density(average.house.price.samples.2016))
cat("Posterior mean of average price in 2014:", mean(average.house.price.samples.2014),"\n")
cat("Posterior mean of average price in 2015:", mean(average.house.price.samples.2015),"\n")
cat("Posterior mean of average price in 2016:", mean(average.house.price.samples.2016),"\n")
#We load the Premier League scores dataset of season 2019/2020
full.data=read.csv("premier_league_2019_20.csv")
head(full.data)
#The code below displays the info file about the dataset from football_data_notes.txt
#fname <- 'football_data_notes.txt'
#s=readChar(fname, file.info(fname)$size)
#cat(s)
y=c(full.data$FTHG, full.data$FTAG)
G=nrow(full.data)
HT_char=as.character(full.data$HomeTeam)
AT_char=as.character(full.data$AwayTeam)
attack=as.factor(c(HT_char,AT_char))
defense=as.factor(c(AT_char,HT_char))
playing.at.home=c(rep(1,G),rep(0,G))
data=data.frame(y,attack,defense,playing.at.home)
m.poisson=inla(formula=y~1+attack+defense+playing.at.home, data=data, family="Poisson",control.compute = list(dic = T))
summary(m.poisson)
#This function computes the points (i.e. league table) from a list of the scores
#(as a 2*G vector, first G elements home team goals for the G games, while (G+1):(2*G) elements are the away team goals)
#home.t and away.t are vectors of length G that describe the home and away teams for each game
compute.points <- function(scores,home.t,away.t){
n.teams=20;
G=380;
points=rep(0,n.teams)
home.team=as.integer(as.factor(home.t))
away.team=as.integer(as.factor(away.t))
#These lines convert the home and away team names for each game into numbers from 1,2...20     (according to alphabetical order of the names)
home.win=(scores[1:G]>scores[(G+1):(2*G)])
draw=(scores[1:G]==scores[(G+1):(2*G)])
away.win=(scores[1:G]<scores[(G+1):(2*G)])
for (g in 1:G){
if(home.win[g]){
points[home.team[g]]=points[home.team[g]]+3
}
if(away.win[g]){
points[away.team[g]]=points[away.team[g]]+3
}
if(draw[g]){
points[home.team[g]]=points[home.team[g]]+1
points[away.team[g]]=points[away.team[g]]+1
}
}
return(points)
}
prob.liverpool.champion=rep(0,7)
for (i in 1:7){
#r denotes the round which we are interested in among (5,10,15,20,25,30,35)
r=5*i
#We create a new dataframe where the matches after round r have NA in their response variables y
data.r=data
data.r[(10*r+1):G,"y"]<-NA
data.r[(G+10*r+1):(2*G),"y"]<-NA
#We fit the Poisson regression model in INLA for this new dataset, including the options needed for sampling
m.poisson.r=inla(formula=y~1+attack+defense+playing.at.home, data=data.r, family="Poisson",
control.predictor = list(compute = TRUE), control.compute = list(config = TRUE))
#We obtain the samples from the linear predictors
nbsamp=1000
samp.r <- inla.posterior.sample(nbsamp, m.poisson.r)
predictors.r=inla.posterior.sample.eval(function(...) {Predictor},
samp.r)
#We will store the scores.samples from the posterior predictive in a matrix scores.samples.r
#First, we fix the values of the scores for the games played up to round r from the original dataset
scores.samples.r=matrix(0,nrow=2*G,ncol=nbsamp)
scores.samples.r[1:(10*r),]=rep(data.r[1:(10*r),"y"],nbsamp)
scores.samples.r[(G+1):(G+10*r),]=rep(data.r[(G+1):(G+10*r),"y"],nbsamp);
#The next step is to sample from the posterior predictive for the scores of the games after round r
#This is done using the linear predictors, first we compute the rates of the Poisson variables (mean of the observations)
#by taking the exponential of the linear predictor
#After this, we sample the variables from the Poisson distribution with these rates
#This has to be done for the goals of the home team and the away team separately
#as we store them in different parts of the matrix, but the method is the same
rates.H=exp(predictors.r[(10*r+1):G,])
vector.rates.H=as.vector(rates.H)
scores.samples.r[(10*r+1):G,]=as.matrix(rpois(length(vector.rates.H),vector.rates.H),nrow=(G-10*r),ncol=nbsamp)
rates.A=exp(predictors.r[(G+10*r+1):(2*G),])
vector.rates.A=as.vector(rates.A)
scores.samples.r[(G+10*r+1):(2*G),]=as.matrix(rpois(length(vector.rates.A),vector.rates.A),nrow=(G-10*r),ncol=nbsamp)
#Now that we have simulated the results for the remaining games from the posterior predictive, we can compute the probability
#of Liverpool being the champion
#At each iteration, we compute the league table (i.e. points for each team) using our compute.points function
#prob.liverpool.champion is computed by taking the average
liverpool.champion=rep(0,nbsamp)
for (it in 1:nbsamp){
points=compute.points(scores.samples.r[,it],home.t=full.data$HomeTeam,away.t=full.data$AwayTeam)
if(points[10]==max(points)){
liverpool.champion[it]=1
}
}
prob.liverpool.champion[i]=mean(liverpool.champion)
}
plot(x=seq(from=5,to=35,by=5),y=prob.liverpool.champion,type="l",main='Posterior probability that Liverpool is champion vs round number',
xlab="Round number",ylab="Posterior probability that Liverpool is the champion")
